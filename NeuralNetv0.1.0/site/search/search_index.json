{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This site contains the project documentation for the neural network with numpy project that provides functions to build neural network using only numpy components github link . Table Of Contents The documentation consists of three separate parts: Tutorials How-To Guides Reference Quickly find what you're looking for depending on your use case by looking at the different pages. This packages provides modules for computing neural networks Modules exported by this package: activation : provides classes for several types of activation functions layers : This modules provides Layer classes architecture : This modules provides neural network architectures init_funcs : This modules provides initialization functions cost : This modules provides classes for several types of cost functions metrics : This modules provides metrics classes db : This modules provides sqlalchemy orm tables and utility objects pipeline : This modules provides functions for data preparation","title":"Neural net docs"},{"location":"#table-of-contents","text":"The documentation consists of three separate parts: Tutorials How-To Guides Reference Quickly find what you're looking for depending on your use case by looking at the different pages. This packages provides modules for computing neural networks Modules exported by this package: activation : provides classes for several types of activation functions layers : This modules provides Layer classes architecture : This modules provides neural network architectures init_funcs : This modules provides initialization functions cost : This modules provides classes for several types of cost functions metrics : This modules provides metrics classes db : This modules provides sqlalchemy orm tables and utility objects pipeline : This modules provides functions for data preparation","title":"Table Of Contents"},{"location":"how-to-guides/","text":"Building neural networks Creating a Sequential Neural Network 1. Import all modules from neural_net package from neural_net import * import numpy 2. Define Your Model ann_sigmoid = architecture.Sequential( [ layers.Fullyconnected(n_in=2,n_out=50,init_method=init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(n_in=50,n_out=1,init_method=init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) 3. Import or create your training dataset n,k = 5000,2 X = numpy.random.uniform(-100,100,size=(n,k)) y =( (X[:, 0]**2 + X[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 4. Train your model ann_sigmoid.train(X,y,metrics=metrics.accuracy) 5. Make predictions ann_sigmoid.predict(X)","title":"How-To Guides"},{"location":"how-to-guides/#building-neural-networks","text":"","title":"Building neural networks"},{"location":"how-to-guides/#creating-a-sequential-neural-network","text":"","title":"Creating a Sequential Neural Network"},{"location":"how-to-guides/#1-import-all-modules-from-neural_net-package","text":"from neural_net import * import numpy","title":"1. Import all modules from neural_net package"},{"location":"how-to-guides/#2-define-your-model","text":"ann_sigmoid = architecture.Sequential( [ layers.Fullyconnected(n_in=2,n_out=50,init_method=init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(n_in=50,n_out=1,init_method=init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy )","title":"2. Define Your Model"},{"location":"how-to-guides/#3-import-or-create-your-training-dataset","text":"n,k = 5000,2 X = numpy.random.uniform(-100,100,size=(n,k)) y =( (X[:, 0]**2 + X[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0","title":"3. Import or create your training dataset"},{"location":"how-to-guides/#4-train-your-model","text":"ann_sigmoid.train(X,y,metrics=metrics.accuracy)","title":"4. Train your model"},{"location":"how-to-guides/#5-make-predictions","text":"ann_sigmoid.predict(X)","title":"5. Make predictions"},{"location":"reference/","text":"Reference Page Table of Contents layers activation functions neural network architectures initialisation functions cost functions metrics database management data preparation functions Class Models used to build other classes Utility functions Introduction This reference page provides an overview of all functions, classes and methods available in the neural_net project Section 1. layers This modules provides Layer classes Fullyconnected Activation Activation Bases: Layer Activation Layer. This layer handles activation for a given activation function Parameters: func ( callable ) \u2013 an activation function like :func: ~activation.\u03c3 Source code in src\\layers.py 29 30 31 32 33 34 35 36 37 38 39 40 class Activation ( Layer ): \"\"\" Activation Layer. This layer handles activation for a given activation function Args: func (callable): an activation function like :func:`~activation.\u03c3` \"\"\" def __init__ ( self , func , * kargs ) -> None : self + locals () Fullyconnected Bases: Layer A fully connected neural network layer. This layer takes an input vector and transforms it linearly using a weights matrix. The product is then subjected to a non-linear activation function. Parameters: n_in ( int ) \u2013 Number of input features. n_out ( int ) \u2013 Number of output features . init_method ( callable ) \u2013 function that initializes weights and takes in as parameters func(n_in,n_out) -> array.shape = (n_in +1, n_out) func ( callable , default: \u03a3 ) \u2013 default is :func: ~activation.\u03a3 Source code in src\\layers.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Fullyconnected ( Layer ): \"\"\" A fully connected neural network layer. This layer takes an input vector and transforms it linearly using a weights matrix. The product is then subjected to a non-linear activation function. Args: n_in (int): Number of input features. n_out (int): Number of output features . init_method (callable): function that initializes weights and takes in as parameters func(n_in,n_out) -> array.shape = (n_in +1, n_out) func (callable): default is :func:`~activation.\u03a3` \"\"\" def __init__ ( self , n_in : int , n_out : int , init_method : callable , func : callable = \u03a3 ) -> None : self + locals () Section 2. activation functions This modules provides classes for several types of activation functions \u03a3 - Linear combination of weights and biases \u03c3 - sigmoid activation Softmax - Softmax activation LeakyReLU - Leaky rectified linear unit activation LeakyReLU Bases: Neurons A class representing the Leaky Rectified Linear Unit (LeakyReLU) activation function. Attributes: leak ( float ) \u2013 The slope coefficient for negative values. Methods: Name Description compute Computes the LeakyReLU activation for input matrix X. pr Computes the derivative of the LeakyReLU function. grad Computes the gradient for backpropagation. Parameters: alpha ( float ) \u2013 The slope coefficient for negative values (default is 0.001). Source code in src\\activation.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 class LeakyReLU ( Neurons ): \"\"\" A class representing the Leaky Rectified Linear Unit (LeakyReLU) activation function. Attributes: leak (float): The slope coefficient for negative values. Methods: compute(X): Computes the LeakyReLU activation for input matrix X. pr(): Computes the derivative of the LeakyReLU function. grad(\u0394): Computes the gradient for backpropagation. Args: alpha (float): The slope coefficient for negative values (default is 0.001). \"\"\" def __init__ ( self , Layer : Layer , leak : float = .001 ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the LeakyReLU function. Returns: numpy.array: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self [ 'leak' ] + ~ neg def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the LeakyReLU activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: LeakyReLU activation result of shape (n, n_out). \"\"\" self . X = X return numpy . maximum ( self [ 'leak' ] * self . X , self . X ) compute ( X ) Computes the LeakyReLU activation for input matrix X. Parameters: X ( array ) \u2013 Input matrix of shape (n, k). Returns: array \u2013 numpy.array: LeakyReLU activation result of shape (n, n_out). Source code in src\\activation.py 194 195 196 197 198 199 200 201 202 203 204 205 def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the LeakyReLU activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: LeakyReLU activation result of shape (n, n_out). \"\"\" self . X = X return numpy . maximum ( self [ 'leak' ] * self . X , self . X ) pr () Computes the derivative of the LeakyReLU function. Returns: array \u2013 numpy.array: Derivative matrix. Source code in src\\activation.py 185 186 187 188 189 190 191 192 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the LeakyReLU function. Returns: numpy.array: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self [ 'leak' ] + ~ neg Softmax Bases: Neurons A class representing the softmax activation function. Methods: Name Description compute Computes the softmax activation for input matrix X. pr Computes the derivative of the softmax function. grad Computes the gradient for backpropagation. Source code in src\\activation.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 class Softmax ( Neurons ): \"\"\" A class representing the softmax activation function. Attributes: None Methods: compute(X): Computes the softmax activation for input matrix X. pr(): Computes the derivative of the softmax function. grad(\u0394): Computes the gradient for backpropagation. Args: None \"\"\" def __init__ ( self , Layer : Layer ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the sigmoid function. Returns: numpy.array: Derivative matrix. \"\"\" return self . probs * ( 1 - self . probs ) def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the softmax activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Softmax activation result of shape (n, n_out). \"\"\" self . X = X self . probs = ( ex := numpy . exp ( self . X )) / ex . sum ( axis = 1 ) . reshape ( - 1 , 1 ) return self . probs compute ( X ) Computes the softmax activation for input matrix X. Parameters: X ( array ) \u2013 Input matrix of shape (n, k). Returns: array \u2013 numpy.array: Softmax activation result of shape (n, n_out). Source code in src\\activation.py 149 150 151 152 153 154 155 156 157 158 159 160 161 def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the softmax activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Softmax activation result of shape (n, n_out). \"\"\" self . X = X self . probs = ( ex := numpy . exp ( self . X )) / ex . sum ( axis = 1 ) . reshape ( - 1 , 1 ) return self . probs pr () Computes the derivative of the sigmoid function. Returns: array \u2013 numpy.array: Derivative matrix. Source code in src\\activation.py 140 141 142 143 144 145 146 147 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the sigmoid function. Returns: numpy.array: Derivative matrix. \"\"\" return self . probs * ( 1 - self . probs ) \u03a3 Bases: Neurons A class representing a linear combination operation. Attributes: W ( array ) \u2013 Weight matrix of shape (k+1, n_out). Methods: Name Description compute Computes the linear combination of input matrix X and bias vector using weight matrix W. pr Computes the derivative of the linear equation with respect to W (matrix X itself). grad Updates weights self.W and computes the new gradient \u0394 for backpropagation. Source code in src\\activation.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class \u03a3 ( Neurons ): \"\"\" A class representing a linear combination operation. Attributes: W (numpy.array): Weight matrix of shape (k+1, n_out). Methods: compute(X): Computes the linear combination of input matrix X and bias vector using weight matrix W. pr(): Computes the derivative of the linear equation with respect to W (matrix X itself). grad(\u0394): Updates weights self.W and computes the new gradient \u0394 for backpropagation. \"\"\" def __init__ ( self , Layer : Layer ) -> None : self + locals () self . W = self . init_method ( self [ 'Layer_n_in' ], self [ 'Layer_n_out' ]) self . Xb = lambda : numpy . c_ [ self . X , numpy . ones (( self . n (), 1 ))] self . instantiateW () self . storeW () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the linear equation (matrix itself). Returns: numpy.array: Derivative matrix. \"\"\" return self . Xb def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Linear combination result of shape (n, n_out). \"\"\" self . X = X return self . Xb () . dot ( self . W ) def grad ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Updates weights self.W and computes the gradient for backpropagation. Args: \u0394 (numpy.array): Gradient from next activation. \"\"\" self - ( self . pr () . T . dot ( \u0394 )) / self . n () self . \u0394 = \u0394 . dot ( self . W [: - 1 ,:] . T ) #-1 to remove biais return self . \u0394 compute ( X ) Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Parameters: X ( array ) \u2013 Input matrix of shape (n, k). Returns: array \u2013 numpy.array: Linear combination result of shape (n, n_out). Source code in src\\activation.py 47 48 49 50 51 52 53 54 55 56 57 58 def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Linear combination result of shape (n, n_out). \"\"\" self . X = X return self . Xb () . dot ( self . W ) grad ( \u0394 ) Updates weights self.W and computes the gradient for backpropagation. Parameters: \u0394 ( array ) \u2013 Gradient from next activation. Source code in src\\activation.py 60 61 62 63 64 65 66 67 68 69 def grad ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Updates weights self.W and computes the gradient for backpropagation. Args: \u0394 (numpy.array): Gradient from next activation. \"\"\" self - ( self . pr () . T . dot ( \u0394 )) / self . n () self . \u0394 = \u0394 . dot ( self . W [: - 1 ,:] . T ) #-1 to remove biais return self . \u0394 pr () Computes the derivative of the linear equation (matrix itself). Returns: array \u2013 numpy.array: Derivative matrix. Source code in src\\activation.py 38 39 40 41 42 43 44 45 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the linear equation (matrix itself). Returns: numpy.array: Derivative matrix. \"\"\" return self . Xb \u03c3 Bases: Neurons A class representing the sigmoid activation function. Attributes: None Methods: compute(X): Computes the sigmoid activation for input matrix X. pr(): Computes the derivative of the sigmoid function. grad(\u0394): Computes the gradient for backpropagation. Args: None Source code in src\\activation.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class \u03c3 ( Neurons ): \"\"\" A class representing the sigmoid activation function. Attributes: None Methods: compute(X): Computes the sigmoid activation for input matrix X. pr(): Computes the derivative of the sigmoid function. grad(\u0394): Computes the gradient for backpropagation. Args: None \"\"\" def __init__ ( self , Layer : Layer ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the sigmoid function. Returns: numpy.array: Derivative matrix. \"\"\" return self . probs * ( 1 - self . probs ) def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the sigmoid activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Sigmoid activation result of shape (n, n_out). \"\"\" self . X = X self . probs = 1 / ( 1 + numpy . exp ( - self . X )) return self . probs compute ( X ) Computes the sigmoid activation for input matrix X. Parameters: X ( array ) \u2013 Input matrix of shape (n, k). Returns: array \u2013 numpy.array: Sigmoid activation result of shape (n, n_out). Source code in src\\activation.py 103 104 105 106 107 108 109 110 111 112 113 114 115 def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the sigmoid activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Sigmoid activation result of shape (n, n_out). \"\"\" self . X = X self . probs = 1 / ( 1 + numpy . exp ( - self . X )) return self . probs pr () Computes the derivative of the sigmoid function. Returns: array \u2013 numpy.array: Derivative matrix. Source code in src\\activation.py 94 95 96 97 98 99 100 101 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the sigmoid function. Returns: numpy.array: Derivative matrix. \"\"\" return self . probs * ( 1 - self . probs ) Section 3. neural network architectures This modules provides neural network architectures Currently available are Sequential - Sequential linear net architecture Sequential Bases: Architecture Source code in src\\architecture.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Sequential ( Architecture ): def __init__ ( self , steps : list [ Layer ], cost : Cost ) -> None : \"\"\" Initialize a Sequential class. Args: steps (List[Layer]): A list of Layer objects representing the steps. cost (Cost): A Cost object for computing cost information. Example: ```python layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) ``` \"\"\" self + locals () self [ 'cost' ] = self [ 'cost' ]( self [ 'id' ]) self . commit () def train ( self , X : numpy . array = None , y : numpy . array = None , batch : Batch = None , epochs : int = 100 , \u03b1 : float = 0.001 , metrics : Metrics = Empty ) -> None : \"\"\" Trains a neural network model using sequential architecture Args: X (numpy.array): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.array): Target variable with shape (n, 1). batch (Optional[Batch]): Optional Batch object that generates batches from the training data. epochs (int): Maximum number of training epochs. \u03b1 (float): Learning rate (step size for weight updates). metrics (Metrics): Metrics object that computes evaluation metrics (e.g., accuracy). Example: ```python from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) ``` \"\"\" Xys = batch or [( X , y )] epochs = tqdm . tqdm ( range ( epochs )) m = metrics () for _ in epochs : for X , y in Xys : self . out = self . predict ( X ) self [ 'cost' ] . compute ( y , self . out ) self . update ( \u03b1 * self [ 'cost' ] . pr ()) epochs . set_description ( ' ' . join ( map ( repr ,[ self [ 'cost' ], self [ 'cost' ] . compute_store () . round ( 4 ), m , m . compute ( y , self . out )]))) self . updateW () self . commit () __init__ ( steps , cost ) Initialize a Sequential class. Parameters: steps ( List [ Layer ] ) \u2013 A list of Layer objects representing the steps. cost ( Cost ) \u2013 A Cost object for computing cost information. Example: layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) Source code in src\\architecture.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , steps : list [ Layer ], cost : Cost ) -> None : \"\"\" Initialize a Sequential class. Args: steps (List[Layer]): A list of Layer objects representing the steps. cost (Cost): A Cost object for computing cost information. Example: ```python layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) ``` \"\"\" self + locals () self [ 'cost' ] = self [ 'cost' ]( self [ 'id' ]) self . commit () train ( X = None , y = None , batch = None , epochs = 100 , \u03b1 = 0.001 , metrics = Empty ) Trains a neural network model using sequential architecture Parameters: X ( array , default: None ) \u2013 Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y ( array , default: None ) \u2013 Target variable with shape (n, 1). batch ( Optional [ Batch ] , default: None ) \u2013 Optional Batch object that generates batches from the training data. epochs ( int , default: 100 ) \u2013 Maximum number of training epochs. \u03b1 ( float , default: 0.001 ) \u2013 Learning rate (step size for weight updates). metrics ( Metrics , default: Empty ) \u2013 Metrics object that computes evaluation metrics (e.g., accuracy). Example: from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) Source code in src\\architecture.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def train ( self , X : numpy . array = None , y : numpy . array = None , batch : Batch = None , epochs : int = 100 , \u03b1 : float = 0.001 , metrics : Metrics = Empty ) -> None : \"\"\" Trains a neural network model using sequential architecture Args: X (numpy.array): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.array): Target variable with shape (n, 1). batch (Optional[Batch]): Optional Batch object that generates batches from the training data. epochs (int): Maximum number of training epochs. \u03b1 (float): Learning rate (step size for weight updates). metrics (Metrics): Metrics object that computes evaluation metrics (e.g., accuracy). Example: ```python from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) ``` \"\"\" Xys = batch or [( X , y )] epochs = tqdm . tqdm ( range ( epochs )) m = metrics () for _ in epochs : for X , y in Xys : self . out = self . predict ( X ) self [ 'cost' ] . compute ( y , self . out ) self . update ( \u03b1 * self [ 'cost' ] . pr ()) epochs . set_description ( ' ' . join ( map ( repr ,[ self [ 'cost' ], self [ 'cost' ] . compute_store () . round ( 4 ), m , m . compute ( y , self . out )]))) self . updateW () self . commit () Section 4. initialisation functions This modules provides initialization functions zeros(n_in: int, n_out: int) - Initializes a weight matrix with zeros XHsigmoiduniform - AA function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. XHReluuniform - A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. XHReluuniform ( n_in , n_out ) A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. Attributes: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). Source code in src\\init_funcs.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def XHReluuniform ( n_in : int , n_out : int ) -> numpy . array : \"\"\" A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. Attributes: n_in (int): Number of input units. n_out (int): Number of output units (neurons). Args: n_in (int): Number of input units. n_out (int): Number of output units (neurons). \"\"\" r = 2 ** .5 * ( 6 / ( n_in + n_out )) ** .5 return numpy . random . uniform ( low =- r , high = r , size = ( n_in + 1 , n_out )) XHsigmoiduniform ( n_in , n_out ) A function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. Attributes: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). Source code in src\\init_funcs.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def XHsigmoiduniform ( n_in : int , n_out : int ) -> numpy . array : \"\"\" A function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. Attributes: n_in (int): Number of input units. n_out (int): Number of output units (neurons). Args: n_in (int): Number of input units. n_out (int): Number of output units (neurons). \"\"\" r = ( 6 / ( n_in + n_out )) ** .5 return numpy . random . uniform ( low =- r , high = r , size = ( n_in + 1 , n_out )) zeros ( n_in , n_out ) Initializes a weight matrix with zeros. Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units. Returns: array \u2013 numpy.array: Weight matrix of shape (n_in + 1, n_out). Source code in src\\init_funcs.py 11 12 13 14 15 16 17 18 19 20 21 22 def zeros ( n_in : int , n_out : int ) -> numpy . array : \"\"\" Initializes a weight matrix with zeros. Args: n_in (int): Number of input units. n_out (int): Number of output units. Returns: numpy.array: Weight matrix of shape (n_in + 1, n_out). \"\"\" return numpy . zeros (( n_in + 1 , n_out )) Section 5. cost functions This modules provides classes for several types of cost functions binaryCrossEntropy CrossEntropy MSE CrossEntropy Bases: Cost Cross-Entropy Loss. This class computes the cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: Name Description - compute numpy.array, p: numpy.array) -> float: Computes the cross-entropy loss. - pr Computes the derivative function values. Example >>> y_true = numpy.array([[1, 0, 0], ... [0, 1, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0]]) >>> predicted_probs = numpy.array([[0, 0.6, 0.3], ... [0.4, 0.2, 0.4], ... [0.2, 0.3, 0.5], ... [0.5, 0.1, 0.4], ... [0.3, 0.4, 0.3]]) >>> ce_loss = CrossEntropy() >>> loss_value = ce_loss.compute(y_true, predicted_probs) >>> print(f\"Cross-Entropy Loss: {loss_value:.4f}\") Cross-Entropy Loss: 1.7915 >>> derivative_values = ce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: array([[-1.00000000e+07, 2.50000000e+00, 1.42857143e+00], [ 1.66666667e+00, -5.00000000e+00, 1.66666667e+00], [ 1.25000000e+00, 1.42857143e+00, -2.00000000e+00], [ 2.00000000e+00, -1.00000000e+01, 1.66666667e+00], [-3.33333333e+00, 1.66666667e+00, 1.42857143e+00]]) Source code in src\\cost.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class CrossEntropy ( Cost ): \"\"\" Cross-Entropy Loss. This class computes the cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: - compute(y: numpy.array, p: numpy.array) -> float: Computes the cross-entropy loss. - pr() -> numpy.array: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[1, 0, 0], ... [0, 1, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0]]) >>> predicted_probs = numpy.array([[0, 0.6, 0.3], ... [0.4, 0.2, 0.4], ... [0.2, 0.3, 0.5], ... [0.5, 0.1, 0.4], ... [0.3, 0.4, 0.3]]) >>> ce_loss = CrossEntropy() >>> loss_value = ce_loss.compute(y_true, predicted_probs) >>> print(f\"Cross-Entropy Loss: {loss_value:.4f}\") Cross-Entropy Loss: 1.7915 >>> derivative_values = ce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: array([[-1.00000000e+07, 2.50000000e+00, 1.42857143e+00], [ 1.66666667e+00, -5.00000000e+00, 1.66666667e+00], [ 1.25000000e+00, 1.42857143e+00, -2.00000000e+00], [ 2.00000000e+00, -1.00000000e+01, 1.66666667e+00], [-3.33333333e+00, 1.66666667e+00, 1.42857143e+00]]) ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.array: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the Cross-entropy loss. Args: y (numpy.array): True labels (0 or 1). p (numpy.array): Predicted probabilities (between 0 and 1). Returns: float: Cross-entropy loss value. \"\"\" self . y , self . p = y , p self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean () compute ( y , p ) Computes the Cross-entropy loss. Parameters: y ( array ) \u2013 True labels (0 or 1). p ( array ) \u2013 Predicted probabilities (between 0 and 1). Returns: float ( float ) \u2013 Cross-entropy loss value. Source code in src\\cost.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the Cross-entropy loss. Args: y (numpy.array): True labels (0 or 1). p (numpy.array): Predicted probabilities (between 0 and 1). Returns: float: Cross-entropy loss value. \"\"\" self . y , self . p = y , p self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean () pr () Computes the derivative function values with respet to p . Returns: array \u2013 numpy.array: Derivative function values. Source code in src\\cost.py 104 105 106 107 108 109 110 111 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.array: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) MSE Bases: Cost Mean Squared Error (MSE) Loss. This class computes the mean squared error loss between true labels (y) and predicted values (p). Methods: Name Description - compute numpy.array, p: numpy.array) -> float: Computes the mean squared error loss. - pr Computes the derivative function values. Example >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mse_loss = MSE() >>> loss_value = mse_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] Source code in src\\cost.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 class MSE ( Cost ): \"\"\" Mean Squared Error (MSE) Loss. This class computes the mean squared error loss between true labels (y) and predicted values (p). Methods: - compute(y: numpy.array, p: numpy.array) -> float: Computes the mean squared error loss. - pr() -> numpy.array: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mse_loss = MSE() >>> loss_value = mse_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.array: Derivative function values. \"\"\" return - 2 * ( self . y - self . p ) def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the mean squared error loss. Args: y (numpy.array): True labels (ground truth). p (numpy.array): Predicted values. Returns: float: Mean squared error loss value. \"\"\" self . y , self . p = y , p return (( self . y - self . p ) ** 2 ) . mean () compute ( y , p ) Computes the mean squared error loss. Parameters: y ( array ) \u2013 True labels (ground truth). p ( array ) \u2013 Predicted values. Returns: float ( float ) \u2013 Mean squared error loss value. Source code in src\\cost.py 166 167 168 169 170 171 172 173 174 175 176 177 178 def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the mean squared error loss. Args: y (numpy.array): True labels (ground truth). p (numpy.array): Predicted values. Returns: float: Mean squared error loss value. \"\"\" self . y , self . p = y , p return (( self . y - self . p ) ** 2 ) . mean () pr () Computes the derivative function values with respet to p . Returns: array \u2013 numpy.array: Derivative function values. Source code in src\\cost.py 158 159 160 161 162 163 164 165 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.array: Derivative function values. \"\"\" return - 2 * ( self . y - self . p ) binaryCrossEntropy Bases: Cost Binary Cross-Entropy Loss. This class computes the binary cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: Name Description - compute numpy.array, p: numpy.array) -> float: Computes the binary cross-entropy loss. - pr numpy.array, p: numpy.array) -> numpy.array: Computes the derivative function values. Example >>> y_true = numpy.array([[0], [1], [1], [0]]) >>> predicted_probs = numpy.array([[0.2], [0.8], [0.6], [0.3]]) >>> bce_loss = binaryCrossEntropy() >>> loss_value = bce_loss.compute(y_true, predicted_probs) >>> print(f\"Binary Cross-Entropy Loss: {loss_value:.4f}\") Binary Cross-Entropy Loss: 0.3284 >>> derivative_values = bce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [ 1.25 -1.25 -1.66666667 1.42857143] Source code in src\\cost.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class binaryCrossEntropy ( Cost ): \"\"\" Binary Cross-Entropy Loss. This class computes the binary cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: - compute(y: numpy.array, p: numpy.array) -> float: Computes the binary cross-entropy loss. - pr(y: numpy.array, p: numpy.array) -> numpy.array: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[0], [1], [1], [0]]) >>> predicted_probs = numpy.array([[0.2], [0.8], [0.6], [0.3]]) >>> bce_loss = binaryCrossEntropy() >>> loss_value = bce_loss.compute(y_true, predicted_probs) >>> print(f\"Binary Cross-Entropy Loss: {loss_value:.4f}\") Binary Cross-Entropy Loss: 0.3284 >>> derivative_values = bce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [ 1.25 -1.25 -1.66666667 1.42857143] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p. Returns: numpy.array: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the binary cross-entropy loss. Args: y (numpy.array): True labels (0 or 1). p (numpy.array): Predicted probabilities (between 0 and 1). Returns: float: Binary cross-entropy loss value. \"\"\" self . y , self . p = y , p self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean () compute ( y , p ) Computes the binary cross-entropy loss. Parameters: y ( array ) \u2013 True labels (0 or 1). p ( array ) \u2013 Predicted probabilities (between 0 and 1). Returns: float ( float ) \u2013 Binary cross-entropy loss value. Source code in src\\cost.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the binary cross-entropy loss. Args: y (numpy.array): True labels (0 or 1). p (numpy.array): Predicted probabilities (between 0 and 1). Returns: float: Binary cross-entropy loss value. \"\"\" self . y , self . p = y , p self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean () pr () Computes the derivative function values with respet to p. Returns: array \u2013 numpy.array: Derivative function values. Source code in src\\cost.py 41 42 43 44 45 46 47 48 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p. Returns: numpy.array: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) Section 6. metrics This modules provides metrics classes accuracy accuracy Bases: Metrics Calculates the accuracy metric for binary or multiclass classification tasks. Parameters: threshold ( float , default: 0.5 ) \u2013 Threshold value for binary classification. Defaults to 0.5. Attributes: threshold ( float ) \u2013 The threshold value used for binary classification. Methods: Name Description compute Computes the accuracy score based on true labels (y) and predicted probabilities (p). Example: >>> acc = accuracy(threshold=0.6) >>> y_true = numpy.array([[1], [0], [1], [0]]) >>> y_pred = numpy.array([[0.8], [0.3], [0.9], [0.5]]) >>> val = acc.compute(y_true, y_pred) >>> print(f\"Accuracy: {val:.4f}\") Accuracy: 1.0000 >>> y_true_multiclass = numpy.array([[0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 1, 0], ... [0, 0, 1]]) >>> y_pred_multiclass = numpy.array([ ... [0.1, 0.2, 0.7], # Predicted probabilities for class 0 ... [0.6, 0.3, 0.1], # Predicted probabilities for class 1 ... [0.8, 0.1, 0.1], # Predicted probabilities for class 2 ... [0.2, 0.3, 0.5], ... [0.4, 0.4, 0.2], ... [0.7, 0.2, 0.1], ... [0.3, 0.4, 0.3], ... [0.1, 0.2, 0.7] ... ]) >>> model_multiclass = accuracy(threshold=0.5) >>> acc_multiclass = model_multiclass.compute(y_true_multiclass, y_pred_multiclass) >>> print(f\"Accuracy (multiclass): {acc_multiclass:.4f}\") Accuracy (multiclass): 0.7500 Source code in src\\metrics.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class accuracy ( Metrics ): \"\"\" Calculates the accuracy metric for binary or multiclass classification tasks. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. Attributes: threshold (float): The threshold value used for binary classification. Methods: compute(y, p): Computes the accuracy score based on true labels (y) and predicted probabilities (p). Example: ```python >>> acc = accuracy(threshold=0.6) >>> y_true = numpy.array([[1], [0], [1], [0]]) >>> y_pred = numpy.array([[0.8], [0.3], [0.9], [0.5]]) >>> val = acc.compute(y_true, y_pred) >>> print(f\"Accuracy: {val:.4f}\") Accuracy: 1.0000 >>> y_true_multiclass = numpy.array([[0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 1, 0], ... [0, 0, 1]]) >>> y_pred_multiclass = numpy.array([ ... [0.1, 0.2, 0.7], # Predicted probabilities for class 0 ... [0.6, 0.3, 0.1], # Predicted probabilities for class 1 ... [0.8, 0.1, 0.1], # Predicted probabilities for class 2 ... [0.2, 0.3, 0.5], ... [0.4, 0.4, 0.2], ... [0.7, 0.2, 0.1], ... [0.3, 0.4, 0.3], ... [0.1, 0.2, 0.7] ... ]) >>> model_multiclass = accuracy(threshold=0.5) >>> acc_multiclass = model_multiclass.compute(y_true_multiclass, y_pred_multiclass) >>> print(f\"Accuracy (multiclass): {acc_multiclass:.4f}\") Accuracy (multiclass): 0.7500 ``` \"\"\" def __init__ ( self , threshold = .5 ) -> None : \"\"\" Initializes the accuracy metric. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. \"\"\" self . threshold = threshold def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the accuracy of predictions. Args: y (numpy.array): True labels (ground truth). p (numpy.array): Predicted values. Returns: float: accuracy value. \"\"\" if y . shape [ 1 ] > 1 : p = p . argmax ( axis = 1 ) y = y . argmax ( axis = 1 ) else : p = ( p > self . threshold ) + 0 self . y , self . p = y , p return (( self . y == self . p ) . sum () / len ( self . y )) . round ( 4 ) __init__ ( threshold = 0.5 ) Initializes the accuracy metric. Parameters: threshold ( float , default: 0.5 ) \u2013 Threshold value for binary classification. Defaults to 0.5. Source code in src\\metrics.py 58 59 60 61 62 63 64 65 def __init__ ( self , threshold = .5 ) -> None : \"\"\" Initializes the accuracy metric. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. \"\"\" self . threshold = threshold compute ( y , p ) Computes the accuracy of predictions. Parameters: y ( array ) \u2013 True labels (ground truth). p ( array ) \u2013 Predicted values. Returns: float ( float ) \u2013 accuracy value. Source code in src\\metrics.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the accuracy of predictions. Args: y (numpy.array): True labels (ground truth). p (numpy.array): Predicted values. Returns: float: accuracy value. \"\"\" if y . shape [ 1 ] > 1 : p = p . argmax ( axis = 1 ) y = y . argmax ( axis = 1 ) else : p = ( p > self . threshold ) + 0 self . y , self . p = y , p return (( self . y == self . p ) . sum () / len ( self . y )) . round ( 4 ) Section 7. database management This modules provides sqlalchemy orm tables and utility objects DefaultTable - generic table template Architecture - Architecture table Layer - layer table Neurons - neurons table Cost - cost table Weight - weight table DBmanager Manages database connections and sessions using SQLAlchemy. Parameters: db ( str ) \u2013 Path to database server or SQLite database file. Defaults to None. Attributes: session ( Session ) \u2013 SQLAlchemy session for database operations. Methods: Name Description _DBmanager__start Starts a session add_table(table: DefaultTable) -> None: Adds a table instance to the current session. commit() -> None: Commits changes to the session. Source code in src\\db.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class DBmanager : \"\"\" Manages database connections and sessions using SQLAlchemy. Args: db (str, optional): Path to database server or SQLite database file. Defaults to None. Attributes: session (Session): SQLAlchemy session for database operations. Methods: _DBmanager__start()-> None: Starts a session add_table(table: DefaultTable) -> None: Adds a table instance to the current session. commit() -> None: Commits changes to the session. \"\"\" engines = {} status = False def __start ( db : str = None ) -> None : db_path = db or f 'sqlite:/// { get_module_path ([ \"run\" , f \"model { now () } .db\" ]) } ' DBmanager . path = db_path DBmanager . engines [ DBmanager . path ] = create_engine ( DBmanager . path ) Base . metadata . create_all ( DBmanager . engines [ DBmanager . path ]) Session = sessionmaker ( bind = DBmanager . engines [ DBmanager . path ]) DBmanager . session = Session () def add_table ( self , table : DefaultTable ) -> None : if not DBmanager . status : DBmanager . _DBmanager__start () DBmanager . status = True DBmanager . session . add ( table ) def commit ( self ) -> None : DBmanager . session . commit () get_instance ( self ) Returns an SQLAlchemy Table object corresponding to the given table name. Returns: Table ( DefaultTable ) \u2013 SQLAlchemy Table object corresponding to the specified table name. Source code in src\\db.py 70 71 72 73 74 75 76 77 78 79 def get_instance ( self ) -> DefaultTable : \"\"\" Returns an SQLAlchemy Table object corresponding to the given table name. Returns: Table: SQLAlchemy Table object corresponding to the specified table name. \"\"\" table , cols = tables [ str ( self )] values = { k : v for k , v in self . id . items () if k in cols } return table ( ** values ) update_instance ( self ) Updates the given instance with the provided keyword arguments. Source code in src\\db.py 81 82 83 84 85 86 87 88 89 def update_instance ( self ) -> None : \"\"\" Updates the given instance with the provided keyword arguments. \"\"\" _ , cols = tables [ str ( self )] for k , v in self . id . items (): if k in cols : setattr ( self . table , k , v ) Section 8. data preparation functions This modules provides functions for data preparation Batch - feed data in chunks shuffle - shuffles train sets onehot - onehot encodes target variables scaler - scales input features Batch Source code in src\\pipeline.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class Batch : def __init__ ( self , size : int , obs : int , X : callable , y : callable ) -> None : \"\"\" Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.array): function providing access to Numpy array containing features. y (numpy.array): function providing access to Numpy array containing target variable. Returns: None Example: ```python >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] ``` \"\"\" self . size = size self . obs = obs self . X = X self . y = y self . getters = lambda ix : ( X ()[ ix ,:], y ()[ ix ,:]) self . i = self . getters ( slice ( 0 , 10 )) self . ix = get_ix ( size , obs ) self . c = 0 def __iter__ ( self ): return self def __next__ ( self ): if self . c < len ( self . ix ): self . c += 1 return self . getters ( self . ix [ self . c - 1 ]) self . c = 0 raise StopIteration __init__ ( size , obs , X , y ) Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.array): function providing access to Numpy array containing features. y (numpy.array): function providing access to Numpy array containing target variable. Returns: None Example: >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] Source code in src\\pipeline.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , size : int , obs : int , X : callable , y : callable ) -> None : \"\"\" Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.array): function providing access to Numpy array containing features. y (numpy.array): function providing access to Numpy array containing target variable. Returns: None Example: ```python >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] ``` \"\"\" self . size = size self . obs = obs self . X = X self . y = y self . getters = lambda ix : ( X ()[ ix ,:], y ()[ ix ,:]) self . i = self . getters ( slice ( 0 , 10 )) self . ix = get_ix ( size , obs ) self . c = 0 get_ix ( size , obs ) Create batch slices for a given sample size and batch size. Parameters: obs ( int ) \u2013 Total number of samples in the dataset. size ( int ) \u2013 Size of each batch. Returns: list [ slice ] \u2013 list[slice]: A list of slice objects representing batch indices. Example: >>> obs = 70 # Total samples >>> batch_size = 20 >>> batch_slices = get_ix(obs, batch_size) >>> batch_slices [slice(0, 20, None), slice(20, 40, None), slice(40, 60, None), slice(60, 70, None)] Source code in src\\pipeline.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def get_ix ( size : int , obs : int ) -> list [ slice ]: \"\"\" Create batch slices for a given sample size and batch size. Args: obs (int): Total number of samples in the dataset. size (int): Size of each batch. Returns: list[slice]: A list of slice objects representing batch indices. Example: ```python >>> obs = 70 # Total samples >>> batch_size = 20 >>> batch_slices = get_ix(obs, batch_size) >>> batch_slices [slice(0, 20, None), slice(20, 40, None), slice(40, 60, None), slice(60, 70, None)] ``` \"\"\" batchix = list ( range ( 0 , obs , size )) if batchix [ - 1 ] < obs : batchix . append ( obs ) batchix = [ slice ( low , high ) for low , high in zip ( batchix , batchix [ 1 :])] return batchix onehot ( y ) One-hot encodes a categorical target variable. Parameters: y ( array ) \u2013 Numpy array containing the categorical target variable. Returns: array \u2013 numpy.array: One-hot encoded representation of the target variable. Example: >>> y = numpy.array([[0],[ 1], [2], [1], [0]]) >>> onehot_encoded = onehot(y) >>> print(onehot_encoded) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] Source code in src\\pipeline.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def onehot ( y : numpy . array ) -> numpy . array : \"\"\" One-hot encodes a categorical target variable. Args: y (numpy.array): Numpy array containing the categorical target variable. Returns: numpy.array: One-hot encoded representation of the target variable. Example: ```python >>> y = numpy.array([[0],[ 1], [2], [1], [0]]) >>> onehot_encoded = onehot(y) >>> print(onehot_encoded) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] ``` \"\"\" return ( y == numpy . unique ( y )) + 0 scaler ( X ) Custom scaler function for centering and standardizing features. Parameters: X ( array ) \u2013 Input numpy array containing features. Returns: array \u2013 numpy.array: Scaled version of the input array. Example: >>> X = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> scaled_X = scaler(X) >>> print(scaled_X) [[-1.22474487 -1.22474487] [ 0. 0. ] [ 1.22474487 1.22474487]] Source code in src\\pipeline.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def scaler ( X : numpy . array ) -> numpy . array : \"\"\" Custom scaler function for centering and standardizing features. Args: X (numpy.array): Input numpy array containing features. Returns: numpy.array: Scaled version of the input array. Example: ```python >>> X = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> scaled_X = scaler(X) >>> print(scaled_X) [[-1.22474487 -1.22474487] [ 0. 0. ] [ 1.22474487 1.22474487]] ``` \"\"\" return ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) shuffle ( X , y ) shuffle features and tagert variable numpy arrays X and y using pandas.sample method. Parameters: X ( array ) \u2013 Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y ( array ) \u2013 Target variable with shape (n, 1). Returns: tuple [ array , array ] \u2013 Tuple[numpy.array, numpy.array]: Shuffled X and y arrays. Example: >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> shuffled_X, shuffled_y = shuffle(X_train, y_train) # Now shuffled_X and shuffled_y contain randomly shuffled samples. Source code in src\\pipeline.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def shuffle ( X : numpy . array , y : numpy . array ) -> tuple [ numpy . array , numpy . array ]: \"\"\" shuffle features and tagert variable numpy arrays X and y using pandas.sample method. Args: X (numpy.array): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.array): Target variable with shape (n, 1). Returns: Tuple[numpy.array, numpy.array]: Shuffled X and y arrays. Example: ```python >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> shuffled_X, shuffled_y = shuffle(X_train, y_train) # Now shuffled_X and shuffled_y contain randomly shuffled samples. ``` \"\"\" X = pandas . DataFrame ( X ) . sample ( frac = 1 ) y = pandas . DataFrame ( y ) . loc [ X . index ] return X . values , y . values 9. Class Models used to build other classes This modules provides genric templates for other modules Define - generic object Architecture - Architecture super object Layer - layer super object Neurons - neurons super object Cost - cost super object Metrics - weight super object Architecture Bases: Define Model for Architecture functions see :func: ~architecture.Sequential Source code in src\\model.py 316 317 318 319 320 321 322 323 class Architecture ( Define ): \"\"\" Model for Architecture functions see :func:`~architecture.Sequential` \"\"\" def __str__ ( self ) -> str : return 'Architecture' Cost Bases: Define Model for Cost functions see :func: ~cost.binaryCrossEntropy Source code in src\\model.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 class Cost ( Define ): \"\"\" Model for Cost functions see :func:`~cost.binaryCrossEntropy` \"\"\" def clip ( self ): \"\"\" Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of ``[0, 1]`` is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``. No check is performed to ensure ``a_min < a_max``. Parameters ---------- a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If ``None``, clipping is not performed on the corresponding edge. Only one of `a_min` and `a_max` may be ``None``. Both are broadcast against `a`. out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. `out` must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`. .. versionadded:: 1.17.0 Returns ------- clipped_array : ndarray An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`. See Also -------- :ref:`ufuncs-output-type` Notes ----- When `a_min` is greater than `a_max`, `clip` returns an array in which all values are equal to `a_max`, as shown in the second example. Examples -------- ```python >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) ``` \"\"\" \u03b5 = 1e-7 self . p = self . p . clip ( \u03b5 , 1 - \u03b5 ) def __str__ ( self ) -> str : return 'Cost' clip () Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than np.minimum(a_max, np.maximum(a, a_min)) . No check is performed to ensure a_min < a_max . Parameters a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If None , clipping is not performed on the corresponding edge. Only one of a_min and a_max may be None . Both are broadcast against a . out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. out must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref: ufunc docs <ufuncs.kwargs> . .. versionadded:: 1.17.0 Returns clipped_array : ndarray An array with the elements of a , but where values < a_min are replaced with a_min , and those > a_max with a_max . See Also :ref: ufuncs-output-type Notes When a_min is greater than a_max , clip returns an array in which all values are equal to a_max , as shown in the second example. Examples >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) Source code in src\\model.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def clip ( self ): \"\"\" Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of ``[0, 1]`` is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``. No check is performed to ensure ``a_min < a_max``. Parameters ---------- a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If ``None``, clipping is not performed on the corresponding edge. Only one of `a_min` and `a_max` may be ``None``. Both are broadcast against `a`. out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. `out` must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`. .. versionadded:: 1.17.0 Returns ------- clipped_array : ndarray An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`. See Also -------- :ref:`ufuncs-output-type` Notes ----- When `a_min` is greater than `a_max`, `clip` returns an array in which all values are equal to `a_max`, as shown in the second example. Examples -------- ```python >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) ``` \"\"\" \u03b5 = 1e-7 self . p = self . p . clip ( \u03b5 , 1 - \u03b5 ) Define Bases: DBmanager Source code in src\\model.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class Define ( DBmanager ): def __repr__ ( self ) -> str : \"\"\" Returns the name of the class. Returns: str: The name of the class. \"\"\" return self . __class__ . __name__ @property def id ( self ) -> dict : return self . _id @id . setter def id ( self , loc ) -> None : \"\"\" Sets id property for the class Instantiate and stores sqlalchemy table of self \"\"\" loc = unfold ( loc ) self . _id = { 'id' : id ( self ), f ' { str ( self ) } _id' : id ( self ), 'name' : repr ( self ), ** loc } if not hasattr ( self , 'table' ): self . table = get_instance ( self ) self . add_table ( self . table ) else : update_instance ( self ) self . commit () def __getitem__ ( self , ix ) -> any : return self . id [ ix ] def __setitem__ ( self , ix , val ) -> None : self . id [ ix ] = val @property def get ( self ) -> dict . get : return self . id . get def __add__ ( self , loc : dict ) -> None : \"\"\" Triggers the id property Args: loc (dict) : dictionary of properties \"\"\" self . id = loc self . c = 0 class func : def __init__ ( self , _ ): ... self . init_method = self . get ( 'Layer_init_method' , func ) self . func = self . get ( 'func' , func ) self . func = self . func ( self . id ) self [ 'steps' ] = self . get ( 'steps' ,[]) parent = { f ' { str ( self ) } _id' : self [ 'id' ]} for step in self : step . id = { ** step . id , ** parent } def __iter__ ( self ) -> object : return self def __len__ ( self ) -> int : return len ( self [ 'steps' ]) def __next__ ( self ) -> any : if self . c < len ( self ): self . c += 1 return self [ 'steps' ][ self . c - 1 ] self . c = 0 raise StopIteration def predict ( self , X : numpy . array ) -> numpy . array : \"\"\" Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Args: X (numpy.array) : input features matrix Returns: numpy.array of output layer predictions \"\"\" self . out = X for step in self : self . out = step . func . compute ( self . out ) return self . out def update ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Implement backpropagation of cost gradient to all layers Passes gradients backward Args: \u0394 (numpy.array) : array of gradient from next step Returns: numpy array of input layer gradient \"\"\" for step in self [ 'steps' ][:: - 1 ]: \u0394 = step . func . grad ( \u0394 ) return \u0394 def compute_store ( self ) -> None : \"\"\" Generic method that computes item and stores it to sqlalchemy session \"\"\" value = self . compute ( self . y , self . p ) self . commit () del ( self . table ) self + { ** self . id , ** locals ()} return value def updateW ( self ) -> None : \"\"\" Updates sqlalchemy tables containing weights \"\"\" for obj in Neurons . with_weights : for i , r in enumerate ( obj . Wtables ): for j , table in enumerate ( r ): setattr ( table , 'value' , obj . W [ i , j ]) __add__ ( loc ) Triggers the id property Parameters: loc ( dict) ) \u2013 dictionary of properties Source code in src\\model.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __add__ ( self , loc : dict ) -> None : \"\"\" Triggers the id property Args: loc (dict) : dictionary of properties \"\"\" self . id = loc self . c = 0 class func : def __init__ ( self , _ ): ... self . init_method = self . get ( 'Layer_init_method' , func ) self . func = self . get ( 'func' , func ) self . func = self . func ( self . id ) self [ 'steps' ] = self . get ( 'steps' ,[]) parent = { f ' { str ( self ) } _id' : self [ 'id' ]} for step in self : step . id = { ** step . id , ** parent } __repr__ () Returns the name of the class. Returns: str ( str ) \u2013 The name of the class. Source code in src\\model.py 17 18 19 20 21 22 23 24 def __repr__ ( self ) -> str : \"\"\" Returns the name of the class. Returns: str: The name of the class. \"\"\" return self . __class__ . __name__ compute_store () Generic method that computes item and stores it to sqlalchemy session Source code in src\\model.py 124 125 126 127 128 129 130 131 132 133 def compute_store ( self ) -> None : \"\"\" Generic method that computes item and stores it to sqlalchemy session \"\"\" value = self . compute ( self . y , self . p ) self . commit () del ( self . table ) self + { ** self . id , ** locals ()} return value predict ( X ) Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Parameters: X ( numpy.array) ) \u2013 input features matrix Returns: array \u2013 numpy.array of output layer predictions Source code in src\\model.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def predict ( self , X : numpy . array ) -> numpy . array : \"\"\" Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Args: X (numpy.array) : input features matrix Returns: numpy.array of output layer predictions \"\"\" self . out = X for step in self : self . out = step . func . compute ( self . out ) return self . out update ( \u0394 ) Implement backpropagation of cost gradient to all layers Passes gradients backward Parameters: \u0394 ( numpy.array) ) \u2013 array of gradient from next step Returns: array \u2013 numpy array of input layer gradient Source code in src\\model.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def update ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Implement backpropagation of cost gradient to all layers Passes gradients backward Args: \u0394 (numpy.array) : array of gradient from next step Returns: numpy array of input layer gradient \"\"\" for step in self [ 'steps' ][:: - 1 ]: \u0394 = step . func . grad ( \u0394 ) return \u0394 updateW () Updates sqlalchemy tables containing weights Source code in src\\model.py 135 136 137 138 139 140 141 142 143 def updateW ( self ) -> None : \"\"\" Updates sqlalchemy tables containing weights \"\"\" for obj in Neurons . with_weights : for i , r in enumerate ( obj . Wtables ): for j , table in enumerate ( r ): setattr ( table , 'value' , obj . W [ i , j ]) Layer Bases: Define Model for layer functions see :func: ~layer.fullyconnected Source code in src\\model.py 145 146 147 148 149 150 151 152 class Layer ( Define ): \"\"\" Model for layer functions see :func:`~layer.fullyconnected` \"\"\" def __str__ ( self ) -> str : return 'Layer' Metrics Bases: Define Model for Metrics functions see :func: ~metrics.accuracy Source code in src\\model.py 307 308 309 310 311 312 313 314 class Metrics ( Define ): \"\"\" Model for Metrics functions see :func:`~metrics.accuracy` \"\"\" def __str__ ( self ) -> str : return 'Metrics' Neurons Bases: Define Model for activation functions see :func: ~activation.Softmax Source code in src\\model.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 class Neurons ( Define ): \"\"\" Model for activation functions see :func:`~activation.Softmax` \"\"\" with_weights = [] def instantiateW ( self ) -> None : \"\"\" Instantiate weight tables \"\"\" table , cols = tables [ 'Weight' ] self . Wtables = [] for i , r in enumerate ( self . W ): instances = [] for j , e in enumerate ( r ): instances += [ table ( Weight_id = f ' { i } _ { j } ' , value = e , Neurons_id = self [ 'id' ] ) ] self . Wtables += [ instances ] instances = [] Neurons . with_weights += [ self ] def storeW ( self ): \"\"\" Stores weights tables \"\"\" for row in self . Wtables : for table in row : self . add_table ( table ) def __str__ ( self ) -> str : return 'Neurons' def __sub__ ( self , \u0394 : numpy . array ) -> None : \"\"\" Substracts Gradient to Weights \"\"\" self . W -= \u0394 def n ( self ) -> int : \"\"\" Returns sample size for current features matrix \"\"\" return self . X . shape [ 0 ] def grad ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Computes gradient for previous step Args: \u0394 (numpy.array) : gradient from next step Returns: numpy.array of gradient for previous step \"\"\" self . \u0394 = self . pr () * \u0394 return self . \u0394 __sub__ ( \u0394 ) Substracts Gradient to Weights Source code in src\\model.py 194 195 196 197 198 199 def __sub__ ( self , \u0394 : numpy . array ) -> None : \"\"\" Substracts Gradient to Weights \"\"\" self . W -= \u0394 grad ( \u0394 ) Computes gradient for previous step Parameters: \u0394 ( numpy.array) ) \u2013 gradient from next step Returns: array \u2013 numpy.array of gradient for previous step Source code in src\\model.py 208 209 210 211 212 213 214 215 216 217 218 219 220 def grad ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Computes gradient for previous step Args: \u0394 (numpy.array) : gradient from next step Returns: numpy.array of gradient for previous step \"\"\" self . \u0394 = self . pr () * \u0394 return self . \u0394 instantiateW () Instantiate weight tables Source code in src\\model.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def instantiateW ( self ) -> None : \"\"\" Instantiate weight tables \"\"\" table , cols = tables [ 'Weight' ] self . Wtables = [] for i , r in enumerate ( self . W ): instances = [] for j , e in enumerate ( r ): instances += [ table ( Weight_id = f ' { i } _ { j } ' , value = e , Neurons_id = self [ 'id' ] ) ] self . Wtables += [ instances ] instances = [] Neurons . with_weights += [ self ] n () Returns sample size for current features matrix Source code in src\\model.py 201 202 203 204 205 def n ( self ) -> int : \"\"\" Returns sample size for current features matrix \"\"\" return self . X . shape [ 0 ] storeW () Stores weights tables Source code in src\\model.py 183 184 185 186 187 188 189 def storeW ( self ): \"\"\" Stores weights tables \"\"\" for row in self . Wtables : for table in row : self . add_table ( table ) 10. Utility functions Provides utility functions get_module_path - Returns the path to a subdirectory named 'dir' relative to the currently executed script. now - current timestamp unfold - Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. get_module_path ( dir ) Returns the path to a subdirectory named 'dir' relative to the currently executed script. Parameters: dir ( str ) \u2013 path to the subdirectory. Returns: str ( str ) \u2013 Absolute path to the specified subdirectory. Source code in src\\utils.py 12 13 14 15 16 17 18 19 20 21 22 def get_module_path ( dir : list [ str ]) -> str : \"\"\" Returns the path to a subdirectory named 'dir' relative to the currently executed script. Args: dir (str): path to the subdirectory. Returns: str: Absolute path to the specified subdirectory. \"\"\" return os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), * dir ) now () Returns the current timestamp as an integer. Returns: int ( int ) \u2013 Current timestamp (number of seconds since the epoch). Source code in src\\utils.py 24 25 26 27 28 29 30 31 def now () -> int : \"\"\" Returns the current timestamp as an integer. Returns: int: Current timestamp (number of seconds since the epoch). \"\"\" return int ( datetime . datetime . now () . timestamp ()) unfold ( d ) Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. Parameters: d ( dict ) \u2013 Input dictionary with nested dictionaries. Returns: dict ( dict ) \u2013 Unfolded dictionary with concatenated keys. Example: >>> d = {'a':1,'b':{'c':2,'d':4}} >>> unfold(d) {'a': 1, 'b_c': 2, 'b_d': 4} Source code in src\\utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def unfold ( d : dict ) -> dict : \"\"\" Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. Args: d (dict): Input dictionary with nested dictionaries. Returns: dict: Unfolded dictionary with concatenated keys. Example: ```python >>> d = {'a':1,'b':{'c':2,'d':4}} >>> unfold(d) {'a': 1, 'b_c': 2, 'b_d': 4} ``` \"\"\" new_d = {} for k in d : if hasattr ( d [ k ], 'keys' ): for j in d [ k ]: new_d [ f ' { k } _ { j } ' ] = d [ k ][ j ] else : new_d [ k ] = d [ k ] return new_d","title":"Reference"},{"location":"reference/#reference-page","text":"","title":"Reference Page"},{"location":"reference/#table-of-contents","text":"layers activation functions neural network architectures initialisation functions cost functions metrics database management data preparation functions Class Models used to build other classes Utility functions","title":"Table of Contents"},{"location":"reference/#introduction","text":"This reference page provides an overview of all functions, classes and methods available in the neural_net project","title":"Introduction"},{"location":"reference/#section-1-layers","text":"This modules provides Layer classes Fullyconnected Activation","title":"Section 1. layers"},{"location":"reference/#src.layers.Activation","text":"Bases: Layer Activation Layer. This layer handles activation for a given activation function Parameters: func ( callable ) \u2013 an activation function like :func: ~activation.\u03c3 Source code in src\\layers.py 29 30 31 32 33 34 35 36 37 38 39 40 class Activation ( Layer ): \"\"\" Activation Layer. This layer handles activation for a given activation function Args: func (callable): an activation function like :func:`~activation.\u03c3` \"\"\" def __init__ ( self , func , * kargs ) -> None : self + locals ()","title":"Activation"},{"location":"reference/#src.layers.Fullyconnected","text":"Bases: Layer A fully connected neural network layer. This layer takes an input vector and transforms it linearly using a weights matrix. The product is then subjected to a non-linear activation function. Parameters: n_in ( int ) \u2013 Number of input features. n_out ( int ) \u2013 Number of output features . init_method ( callable ) \u2013 function that initializes weights and takes in as parameters func(n_in,n_out) -> array.shape = (n_in +1, n_out) func ( callable , default: \u03a3 ) \u2013 default is :func: ~activation.\u03a3 Source code in src\\layers.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Fullyconnected ( Layer ): \"\"\" A fully connected neural network layer. This layer takes an input vector and transforms it linearly using a weights matrix. The product is then subjected to a non-linear activation function. Args: n_in (int): Number of input features. n_out (int): Number of output features . init_method (callable): function that initializes weights and takes in as parameters func(n_in,n_out) -> array.shape = (n_in +1, n_out) func (callable): default is :func:`~activation.\u03a3` \"\"\" def __init__ ( self , n_in : int , n_out : int , init_method : callable , func : callable = \u03a3 ) -> None : self + locals ()","title":"Fullyconnected"},{"location":"reference/#section-2-activation-functions","text":"This modules provides classes for several types of activation functions \u03a3 - Linear combination of weights and biases \u03c3 - sigmoid activation Softmax - Softmax activation LeakyReLU - Leaky rectified linear unit activation","title":"Section 2. activation functions"},{"location":"reference/#src.activation.LeakyReLU","text":"Bases: Neurons A class representing the Leaky Rectified Linear Unit (LeakyReLU) activation function. Attributes: leak ( float ) \u2013 The slope coefficient for negative values. Methods: Name Description compute Computes the LeakyReLU activation for input matrix X. pr Computes the derivative of the LeakyReLU function. grad Computes the gradient for backpropagation. Parameters: alpha ( float ) \u2013 The slope coefficient for negative values (default is 0.001). Source code in src\\activation.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 class LeakyReLU ( Neurons ): \"\"\" A class representing the Leaky Rectified Linear Unit (LeakyReLU) activation function. Attributes: leak (float): The slope coefficient for negative values. Methods: compute(X): Computes the LeakyReLU activation for input matrix X. pr(): Computes the derivative of the LeakyReLU function. grad(\u0394): Computes the gradient for backpropagation. Args: alpha (float): The slope coefficient for negative values (default is 0.001). \"\"\" def __init__ ( self , Layer : Layer , leak : float = .001 ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the LeakyReLU function. Returns: numpy.array: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self [ 'leak' ] + ~ neg def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the LeakyReLU activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: LeakyReLU activation result of shape (n, n_out). \"\"\" self . X = X return numpy . maximum ( self [ 'leak' ] * self . X , self . X )","title":"LeakyReLU"},{"location":"reference/#src.activation.LeakyReLU.compute","text":"Computes the LeakyReLU activation for input matrix X. Parameters: X ( array ) \u2013 Input matrix of shape (n, k). Returns: array \u2013 numpy.array: LeakyReLU activation result of shape (n, n_out). Source code in src\\activation.py 194 195 196 197 198 199 200 201 202 203 204 205 def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the LeakyReLU activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: LeakyReLU activation result of shape (n, n_out). \"\"\" self . X = X return numpy . maximum ( self [ 'leak' ] * self . X , self . X )","title":"compute"},{"location":"reference/#src.activation.LeakyReLU.pr","text":"Computes the derivative of the LeakyReLU function. Returns: array \u2013 numpy.array: Derivative matrix. Source code in src\\activation.py 185 186 187 188 189 190 191 192 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the LeakyReLU function. Returns: numpy.array: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self [ 'leak' ] + ~ neg","title":"pr"},{"location":"reference/#src.activation.Softmax","text":"Bases: Neurons A class representing the softmax activation function. Methods: Name Description compute Computes the softmax activation for input matrix X. pr Computes the derivative of the softmax function. grad Computes the gradient for backpropagation. Source code in src\\activation.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 class Softmax ( Neurons ): \"\"\" A class representing the softmax activation function. Attributes: None Methods: compute(X): Computes the softmax activation for input matrix X. pr(): Computes the derivative of the softmax function. grad(\u0394): Computes the gradient for backpropagation. Args: None \"\"\" def __init__ ( self , Layer : Layer ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the sigmoid function. Returns: numpy.array: Derivative matrix. \"\"\" return self . probs * ( 1 - self . probs ) def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the softmax activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Softmax activation result of shape (n, n_out). \"\"\" self . X = X self . probs = ( ex := numpy . exp ( self . X )) / ex . sum ( axis = 1 ) . reshape ( - 1 , 1 ) return self . probs","title":"Softmax"},{"location":"reference/#src.activation.Softmax.compute","text":"Computes the softmax activation for input matrix X. Parameters: X ( array ) \u2013 Input matrix of shape (n, k). Returns: array \u2013 numpy.array: Softmax activation result of shape (n, n_out). Source code in src\\activation.py 149 150 151 152 153 154 155 156 157 158 159 160 161 def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the softmax activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Softmax activation result of shape (n, n_out). \"\"\" self . X = X self . probs = ( ex := numpy . exp ( self . X )) / ex . sum ( axis = 1 ) . reshape ( - 1 , 1 ) return self . probs","title":"compute"},{"location":"reference/#src.activation.Softmax.pr","text":"Computes the derivative of the sigmoid function. Returns: array \u2013 numpy.array: Derivative matrix. Source code in src\\activation.py 140 141 142 143 144 145 146 147 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the sigmoid function. Returns: numpy.array: Derivative matrix. \"\"\" return self . probs * ( 1 - self . probs )","title":"pr"},{"location":"reference/#src.activation.\u03a3","text":"Bases: Neurons A class representing a linear combination operation. Attributes: W ( array ) \u2013 Weight matrix of shape (k+1, n_out). Methods: Name Description compute Computes the linear combination of input matrix X and bias vector using weight matrix W. pr Computes the derivative of the linear equation with respect to W (matrix X itself). grad Updates weights self.W and computes the new gradient \u0394 for backpropagation. Source code in src\\activation.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class \u03a3 ( Neurons ): \"\"\" A class representing a linear combination operation. Attributes: W (numpy.array): Weight matrix of shape (k+1, n_out). Methods: compute(X): Computes the linear combination of input matrix X and bias vector using weight matrix W. pr(): Computes the derivative of the linear equation with respect to W (matrix X itself). grad(\u0394): Updates weights self.W and computes the new gradient \u0394 for backpropagation. \"\"\" def __init__ ( self , Layer : Layer ) -> None : self + locals () self . W = self . init_method ( self [ 'Layer_n_in' ], self [ 'Layer_n_out' ]) self . Xb = lambda : numpy . c_ [ self . X , numpy . ones (( self . n (), 1 ))] self . instantiateW () self . storeW () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the linear equation (matrix itself). Returns: numpy.array: Derivative matrix. \"\"\" return self . Xb def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Linear combination result of shape (n, n_out). \"\"\" self . X = X return self . Xb () . dot ( self . W ) def grad ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Updates weights self.W and computes the gradient for backpropagation. Args: \u0394 (numpy.array): Gradient from next activation. \"\"\" self - ( self . pr () . T . dot ( \u0394 )) / self . n () self . \u0394 = \u0394 . dot ( self . W [: - 1 ,:] . T ) #-1 to remove biais return self . \u0394","title":"\u03a3"},{"location":"reference/#src.activation.\u03a3.compute","text":"Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Parameters: X ( array ) \u2013 Input matrix of shape (n, k). Returns: array \u2013 numpy.array: Linear combination result of shape (n, n_out). Source code in src\\activation.py 47 48 49 50 51 52 53 54 55 56 57 58 def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Linear combination result of shape (n, n_out). \"\"\" self . X = X return self . Xb () . dot ( self . W )","title":"compute"},{"location":"reference/#src.activation.\u03a3.grad","text":"Updates weights self.W and computes the gradient for backpropagation. Parameters: \u0394 ( array ) \u2013 Gradient from next activation. Source code in src\\activation.py 60 61 62 63 64 65 66 67 68 69 def grad ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Updates weights self.W and computes the gradient for backpropagation. Args: \u0394 (numpy.array): Gradient from next activation. \"\"\" self - ( self . pr () . T . dot ( \u0394 )) / self . n () self . \u0394 = \u0394 . dot ( self . W [: - 1 ,:] . T ) #-1 to remove biais return self . \u0394","title":"grad"},{"location":"reference/#src.activation.\u03a3.pr","text":"Computes the derivative of the linear equation (matrix itself). Returns: array \u2013 numpy.array: Derivative matrix. Source code in src\\activation.py 38 39 40 41 42 43 44 45 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the linear equation (matrix itself). Returns: numpy.array: Derivative matrix. \"\"\" return self . Xb","title":"pr"},{"location":"reference/#src.activation.\u03c3","text":"Bases: Neurons A class representing the sigmoid activation function. Attributes: None Methods: compute(X): Computes the sigmoid activation for input matrix X. pr(): Computes the derivative of the sigmoid function. grad(\u0394): Computes the gradient for backpropagation. Args: None Source code in src\\activation.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class \u03c3 ( Neurons ): \"\"\" A class representing the sigmoid activation function. Attributes: None Methods: compute(X): Computes the sigmoid activation for input matrix X. pr(): Computes the derivative of the sigmoid function. grad(\u0394): Computes the gradient for backpropagation. Args: None \"\"\" def __init__ ( self , Layer : Layer ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the sigmoid function. Returns: numpy.array: Derivative matrix. \"\"\" return self . probs * ( 1 - self . probs ) def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the sigmoid activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Sigmoid activation result of shape (n, n_out). \"\"\" self . X = X self . probs = 1 / ( 1 + numpy . exp ( - self . X )) return self . probs","title":"\u03c3"},{"location":"reference/#src.activation.\u03c3.compute","text":"Computes the sigmoid activation for input matrix X. Parameters: X ( array ) \u2013 Input matrix of shape (n, k). Returns: array \u2013 numpy.array: Sigmoid activation result of shape (n, n_out). Source code in src\\activation.py 103 104 105 106 107 108 109 110 111 112 113 114 115 def compute ( self , X : numpy . array ) -> numpy . array : \"\"\" Computes the sigmoid activation for input matrix X. Args: X (numpy.array): Input matrix of shape (n, k). Returns: numpy.array: Sigmoid activation result of shape (n, n_out). \"\"\" self . X = X self . probs = 1 / ( 1 + numpy . exp ( - self . X )) return self . probs","title":"compute"},{"location":"reference/#src.activation.\u03c3.pr","text":"Computes the derivative of the sigmoid function. Returns: array \u2013 numpy.array: Derivative matrix. Source code in src\\activation.py 94 95 96 97 98 99 100 101 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative of the sigmoid function. Returns: numpy.array: Derivative matrix. \"\"\" return self . probs * ( 1 - self . probs )","title":"pr"},{"location":"reference/#section-3-neural-network-architectures","text":"This modules provides neural network architectures Currently available are Sequential - Sequential linear net architecture","title":"Section 3. neural network architectures"},{"location":"reference/#src.architecture.Sequential","text":"Bases: Architecture Source code in src\\architecture.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Sequential ( Architecture ): def __init__ ( self , steps : list [ Layer ], cost : Cost ) -> None : \"\"\" Initialize a Sequential class. Args: steps (List[Layer]): A list of Layer objects representing the steps. cost (Cost): A Cost object for computing cost information. Example: ```python layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) ``` \"\"\" self + locals () self [ 'cost' ] = self [ 'cost' ]( self [ 'id' ]) self . commit () def train ( self , X : numpy . array = None , y : numpy . array = None , batch : Batch = None , epochs : int = 100 , \u03b1 : float = 0.001 , metrics : Metrics = Empty ) -> None : \"\"\" Trains a neural network model using sequential architecture Args: X (numpy.array): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.array): Target variable with shape (n, 1). batch (Optional[Batch]): Optional Batch object that generates batches from the training data. epochs (int): Maximum number of training epochs. \u03b1 (float): Learning rate (step size for weight updates). metrics (Metrics): Metrics object that computes evaluation metrics (e.g., accuracy). Example: ```python from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) ``` \"\"\" Xys = batch or [( X , y )] epochs = tqdm . tqdm ( range ( epochs )) m = metrics () for _ in epochs : for X , y in Xys : self . out = self . predict ( X ) self [ 'cost' ] . compute ( y , self . out ) self . update ( \u03b1 * self [ 'cost' ] . pr ()) epochs . set_description ( ' ' . join ( map ( repr ,[ self [ 'cost' ], self [ 'cost' ] . compute_store () . round ( 4 ), m , m . compute ( y , self . out )]))) self . updateW () self . commit ()","title":"Sequential"},{"location":"reference/#src.architecture.Sequential.__init__","text":"Initialize a Sequential class. Parameters: steps ( List [ Layer ] ) \u2013 A list of Layer objects representing the steps. cost ( Cost ) \u2013 A Cost object for computing cost information. Example: layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) Source code in src\\architecture.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , steps : list [ Layer ], cost : Cost ) -> None : \"\"\" Initialize a Sequential class. Args: steps (List[Layer]): A list of Layer objects representing the steps. cost (Cost): A Cost object for computing cost information. Example: ```python layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) ``` \"\"\" self + locals () self [ 'cost' ] = self [ 'cost' ]( self [ 'id' ]) self . commit ()","title":"__init__"},{"location":"reference/#src.architecture.Sequential.train","text":"Trains a neural network model using sequential architecture Parameters: X ( array , default: None ) \u2013 Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y ( array , default: None ) \u2013 Target variable with shape (n, 1). batch ( Optional [ Batch ] , default: None ) \u2013 Optional Batch object that generates batches from the training data. epochs ( int , default: 100 ) \u2013 Maximum number of training epochs. \u03b1 ( float , default: 0.001 ) \u2013 Learning rate (step size for weight updates). metrics ( Metrics , default: Empty ) \u2013 Metrics object that computes evaluation metrics (e.g., accuracy). Example: from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) Source code in src\\architecture.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def train ( self , X : numpy . array = None , y : numpy . array = None , batch : Batch = None , epochs : int = 100 , \u03b1 : float = 0.001 , metrics : Metrics = Empty ) -> None : \"\"\" Trains a neural network model using sequential architecture Args: X (numpy.array): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.array): Target variable with shape (n, 1). batch (Optional[Batch]): Optional Batch object that generates batches from the training data. epochs (int): Maximum number of training epochs. \u03b1 (float): Learning rate (step size for weight updates). metrics (Metrics): Metrics object that computes evaluation metrics (e.g., accuracy). Example: ```python from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) ``` \"\"\" Xys = batch or [( X , y )] epochs = tqdm . tqdm ( range ( epochs )) m = metrics () for _ in epochs : for X , y in Xys : self . out = self . predict ( X ) self [ 'cost' ] . compute ( y , self . out ) self . update ( \u03b1 * self [ 'cost' ] . pr ()) epochs . set_description ( ' ' . join ( map ( repr ,[ self [ 'cost' ], self [ 'cost' ] . compute_store () . round ( 4 ), m , m . compute ( y , self . out )]))) self . updateW () self . commit ()","title":"train"},{"location":"reference/#section-4-initialisation-functions","text":"This modules provides initialization functions zeros(n_in: int, n_out: int) - Initializes a weight matrix with zeros XHsigmoiduniform - AA function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. XHReluuniform - A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions.","title":"Section 4. initialisation functions"},{"location":"reference/#src.init_funcs.XHReluuniform","text":"A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. Attributes: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). Source code in src\\init_funcs.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def XHReluuniform ( n_in : int , n_out : int ) -> numpy . array : \"\"\" A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. Attributes: n_in (int): Number of input units. n_out (int): Number of output units (neurons). Args: n_in (int): Number of input units. n_out (int): Number of output units (neurons). \"\"\" r = 2 ** .5 * ( 6 / ( n_in + n_out )) ** .5 return numpy . random . uniform ( low =- r , high = r , size = ( n_in + 1 , n_out ))","title":"XHReluuniform"},{"location":"reference/#src.init_funcs.XHsigmoiduniform","text":"A function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. Attributes: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). Source code in src\\init_funcs.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def XHsigmoiduniform ( n_in : int , n_out : int ) -> numpy . array : \"\"\" A function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. Attributes: n_in (int): Number of input units. n_out (int): Number of output units (neurons). Args: n_in (int): Number of input units. n_out (int): Number of output units (neurons). \"\"\" r = ( 6 / ( n_in + n_out )) ** .5 return numpy . random . uniform ( low =- r , high = r , size = ( n_in + 1 , n_out ))","title":"XHsigmoiduniform"},{"location":"reference/#src.init_funcs.zeros","text":"Initializes a weight matrix with zeros. Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units. Returns: array \u2013 numpy.array: Weight matrix of shape (n_in + 1, n_out). Source code in src\\init_funcs.py 11 12 13 14 15 16 17 18 19 20 21 22 def zeros ( n_in : int , n_out : int ) -> numpy . array : \"\"\" Initializes a weight matrix with zeros. Args: n_in (int): Number of input units. n_out (int): Number of output units. Returns: numpy.array: Weight matrix of shape (n_in + 1, n_out). \"\"\" return numpy . zeros (( n_in + 1 , n_out ))","title":"zeros"},{"location":"reference/#section-5-cost-functions","text":"This modules provides classes for several types of cost functions binaryCrossEntropy CrossEntropy MSE","title":"Section 5. cost functions"},{"location":"reference/#src.cost.CrossEntropy","text":"Bases: Cost Cross-Entropy Loss. This class computes the cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: Name Description - compute numpy.array, p: numpy.array) -> float: Computes the cross-entropy loss. - pr Computes the derivative function values. Example >>> y_true = numpy.array([[1, 0, 0], ... [0, 1, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0]]) >>> predicted_probs = numpy.array([[0, 0.6, 0.3], ... [0.4, 0.2, 0.4], ... [0.2, 0.3, 0.5], ... [0.5, 0.1, 0.4], ... [0.3, 0.4, 0.3]]) >>> ce_loss = CrossEntropy() >>> loss_value = ce_loss.compute(y_true, predicted_probs) >>> print(f\"Cross-Entropy Loss: {loss_value:.4f}\") Cross-Entropy Loss: 1.7915 >>> derivative_values = ce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: array([[-1.00000000e+07, 2.50000000e+00, 1.42857143e+00], [ 1.66666667e+00, -5.00000000e+00, 1.66666667e+00], [ 1.25000000e+00, 1.42857143e+00, -2.00000000e+00], [ 2.00000000e+00, -1.00000000e+01, 1.66666667e+00], [-3.33333333e+00, 1.66666667e+00, 1.42857143e+00]]) Source code in src\\cost.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class CrossEntropy ( Cost ): \"\"\" Cross-Entropy Loss. This class computes the cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: - compute(y: numpy.array, p: numpy.array) -> float: Computes the cross-entropy loss. - pr() -> numpy.array: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[1, 0, 0], ... [0, 1, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0]]) >>> predicted_probs = numpy.array([[0, 0.6, 0.3], ... [0.4, 0.2, 0.4], ... [0.2, 0.3, 0.5], ... [0.5, 0.1, 0.4], ... [0.3, 0.4, 0.3]]) >>> ce_loss = CrossEntropy() >>> loss_value = ce_loss.compute(y_true, predicted_probs) >>> print(f\"Cross-Entropy Loss: {loss_value:.4f}\") Cross-Entropy Loss: 1.7915 >>> derivative_values = ce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: array([[-1.00000000e+07, 2.50000000e+00, 1.42857143e+00], [ 1.66666667e+00, -5.00000000e+00, 1.66666667e+00], [ 1.25000000e+00, 1.42857143e+00, -2.00000000e+00], [ 2.00000000e+00, -1.00000000e+01, 1.66666667e+00], [-3.33333333e+00, 1.66666667e+00, 1.42857143e+00]]) ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.array: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the Cross-entropy loss. Args: y (numpy.array): True labels (0 or 1). p (numpy.array): Predicted probabilities (between 0 and 1). Returns: float: Cross-entropy loss value. \"\"\" self . y , self . p = y , p self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean ()","title":"CrossEntropy"},{"location":"reference/#src.cost.CrossEntropy.compute","text":"Computes the Cross-entropy loss. Parameters: y ( array ) \u2013 True labels (0 or 1). p ( array ) \u2013 Predicted probabilities (between 0 and 1). Returns: float ( float ) \u2013 Cross-entropy loss value. Source code in src\\cost.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the Cross-entropy loss. Args: y (numpy.array): True labels (0 or 1). p (numpy.array): Predicted probabilities (between 0 and 1). Returns: float: Cross-entropy loss value. \"\"\" self . y , self . p = y , p self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean ()","title":"compute"},{"location":"reference/#src.cost.CrossEntropy.pr","text":"Computes the derivative function values with respet to p . Returns: array \u2013 numpy.array: Derivative function values. Source code in src\\cost.py 104 105 106 107 108 109 110 111 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.array: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p ))","title":"pr"},{"location":"reference/#src.cost.MSE","text":"Bases: Cost Mean Squared Error (MSE) Loss. This class computes the mean squared error loss between true labels (y) and predicted values (p). Methods: Name Description - compute numpy.array, p: numpy.array) -> float: Computes the mean squared error loss. - pr Computes the derivative function values. Example >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mse_loss = MSE() >>> loss_value = mse_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] Source code in src\\cost.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 class MSE ( Cost ): \"\"\" Mean Squared Error (MSE) Loss. This class computes the mean squared error loss between true labels (y) and predicted values (p). Methods: - compute(y: numpy.array, p: numpy.array) -> float: Computes the mean squared error loss. - pr() -> numpy.array: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mse_loss = MSE() >>> loss_value = mse_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.array: Derivative function values. \"\"\" return - 2 * ( self . y - self . p ) def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the mean squared error loss. Args: y (numpy.array): True labels (ground truth). p (numpy.array): Predicted values. Returns: float: Mean squared error loss value. \"\"\" self . y , self . p = y , p return (( self . y - self . p ) ** 2 ) . mean ()","title":"MSE"},{"location":"reference/#src.cost.MSE.compute","text":"Computes the mean squared error loss. Parameters: y ( array ) \u2013 True labels (ground truth). p ( array ) \u2013 Predicted values. Returns: float ( float ) \u2013 Mean squared error loss value. Source code in src\\cost.py 166 167 168 169 170 171 172 173 174 175 176 177 178 def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the mean squared error loss. Args: y (numpy.array): True labels (ground truth). p (numpy.array): Predicted values. Returns: float: Mean squared error loss value. \"\"\" self . y , self . p = y , p return (( self . y - self . p ) ** 2 ) . mean ()","title":"compute"},{"location":"reference/#src.cost.MSE.pr","text":"Computes the derivative function values with respet to p . Returns: array \u2013 numpy.array: Derivative function values. Source code in src\\cost.py 158 159 160 161 162 163 164 165 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.array: Derivative function values. \"\"\" return - 2 * ( self . y - self . p )","title":"pr"},{"location":"reference/#src.cost.binaryCrossEntropy","text":"Bases: Cost Binary Cross-Entropy Loss. This class computes the binary cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: Name Description - compute numpy.array, p: numpy.array) -> float: Computes the binary cross-entropy loss. - pr numpy.array, p: numpy.array) -> numpy.array: Computes the derivative function values. Example >>> y_true = numpy.array([[0], [1], [1], [0]]) >>> predicted_probs = numpy.array([[0.2], [0.8], [0.6], [0.3]]) >>> bce_loss = binaryCrossEntropy() >>> loss_value = bce_loss.compute(y_true, predicted_probs) >>> print(f\"Binary Cross-Entropy Loss: {loss_value:.4f}\") Binary Cross-Entropy Loss: 0.3284 >>> derivative_values = bce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [ 1.25 -1.25 -1.66666667 1.42857143] Source code in src\\cost.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class binaryCrossEntropy ( Cost ): \"\"\" Binary Cross-Entropy Loss. This class computes the binary cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: - compute(y: numpy.array, p: numpy.array) -> float: Computes the binary cross-entropy loss. - pr(y: numpy.array, p: numpy.array) -> numpy.array: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[0], [1], [1], [0]]) >>> predicted_probs = numpy.array([[0.2], [0.8], [0.6], [0.3]]) >>> bce_loss = binaryCrossEntropy() >>> loss_value = bce_loss.compute(y_true, predicted_probs) >>> print(f\"Binary Cross-Entropy Loss: {loss_value:.4f}\") Binary Cross-Entropy Loss: 0.3284 >>> derivative_values = bce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [ 1.25 -1.25 -1.66666667 1.42857143] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p. Returns: numpy.array: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the binary cross-entropy loss. Args: y (numpy.array): True labels (0 or 1). p (numpy.array): Predicted probabilities (between 0 and 1). Returns: float: Binary cross-entropy loss value. \"\"\" self . y , self . p = y , p self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean ()","title":"binaryCrossEntropy"},{"location":"reference/#src.cost.binaryCrossEntropy.compute","text":"Computes the binary cross-entropy loss. Parameters: y ( array ) \u2013 True labels (0 or 1). p ( array ) \u2013 Predicted probabilities (between 0 and 1). Returns: float ( float ) \u2013 Binary cross-entropy loss value. Source code in src\\cost.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the binary cross-entropy loss. Args: y (numpy.array): True labels (0 or 1). p (numpy.array): Predicted probabilities (between 0 and 1). Returns: float: Binary cross-entropy loss value. \"\"\" self . y , self . p = y , p self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean ()","title":"compute"},{"location":"reference/#src.cost.binaryCrossEntropy.pr","text":"Computes the derivative function values with respet to p. Returns: array \u2013 numpy.array: Derivative function values. Source code in src\\cost.py 41 42 43 44 45 46 47 48 def pr ( self ) -> numpy . array : \"\"\" Computes the derivative function values with respet to p. Returns: numpy.array: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p ))","title":"pr"},{"location":"reference/#section-6-metrics","text":"This modules provides metrics classes accuracy","title":"Section 6. metrics"},{"location":"reference/#src.metrics.accuracy","text":"Bases: Metrics Calculates the accuracy metric for binary or multiclass classification tasks. Parameters: threshold ( float , default: 0.5 ) \u2013 Threshold value for binary classification. Defaults to 0.5. Attributes: threshold ( float ) \u2013 The threshold value used for binary classification. Methods: Name Description compute Computes the accuracy score based on true labels (y) and predicted probabilities (p). Example: >>> acc = accuracy(threshold=0.6) >>> y_true = numpy.array([[1], [0], [1], [0]]) >>> y_pred = numpy.array([[0.8], [0.3], [0.9], [0.5]]) >>> val = acc.compute(y_true, y_pred) >>> print(f\"Accuracy: {val:.4f}\") Accuracy: 1.0000 >>> y_true_multiclass = numpy.array([[0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 1, 0], ... [0, 0, 1]]) >>> y_pred_multiclass = numpy.array([ ... [0.1, 0.2, 0.7], # Predicted probabilities for class 0 ... [0.6, 0.3, 0.1], # Predicted probabilities for class 1 ... [0.8, 0.1, 0.1], # Predicted probabilities for class 2 ... [0.2, 0.3, 0.5], ... [0.4, 0.4, 0.2], ... [0.7, 0.2, 0.1], ... [0.3, 0.4, 0.3], ... [0.1, 0.2, 0.7] ... ]) >>> model_multiclass = accuracy(threshold=0.5) >>> acc_multiclass = model_multiclass.compute(y_true_multiclass, y_pred_multiclass) >>> print(f\"Accuracy (multiclass): {acc_multiclass:.4f}\") Accuracy (multiclass): 0.7500 Source code in src\\metrics.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class accuracy ( Metrics ): \"\"\" Calculates the accuracy metric for binary or multiclass classification tasks. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. Attributes: threshold (float): The threshold value used for binary classification. Methods: compute(y, p): Computes the accuracy score based on true labels (y) and predicted probabilities (p). Example: ```python >>> acc = accuracy(threshold=0.6) >>> y_true = numpy.array([[1], [0], [1], [0]]) >>> y_pred = numpy.array([[0.8], [0.3], [0.9], [0.5]]) >>> val = acc.compute(y_true, y_pred) >>> print(f\"Accuracy: {val:.4f}\") Accuracy: 1.0000 >>> y_true_multiclass = numpy.array([[0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 1, 0], ... [0, 0, 1]]) >>> y_pred_multiclass = numpy.array([ ... [0.1, 0.2, 0.7], # Predicted probabilities for class 0 ... [0.6, 0.3, 0.1], # Predicted probabilities for class 1 ... [0.8, 0.1, 0.1], # Predicted probabilities for class 2 ... [0.2, 0.3, 0.5], ... [0.4, 0.4, 0.2], ... [0.7, 0.2, 0.1], ... [0.3, 0.4, 0.3], ... [0.1, 0.2, 0.7] ... ]) >>> model_multiclass = accuracy(threshold=0.5) >>> acc_multiclass = model_multiclass.compute(y_true_multiclass, y_pred_multiclass) >>> print(f\"Accuracy (multiclass): {acc_multiclass:.4f}\") Accuracy (multiclass): 0.7500 ``` \"\"\" def __init__ ( self , threshold = .5 ) -> None : \"\"\" Initializes the accuracy metric. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. \"\"\" self . threshold = threshold def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the accuracy of predictions. Args: y (numpy.array): True labels (ground truth). p (numpy.array): Predicted values. Returns: float: accuracy value. \"\"\" if y . shape [ 1 ] > 1 : p = p . argmax ( axis = 1 ) y = y . argmax ( axis = 1 ) else : p = ( p > self . threshold ) + 0 self . y , self . p = y , p return (( self . y == self . p ) . sum () / len ( self . y )) . round ( 4 )","title":"accuracy"},{"location":"reference/#src.metrics.accuracy.__init__","text":"Initializes the accuracy metric. Parameters: threshold ( float , default: 0.5 ) \u2013 Threshold value for binary classification. Defaults to 0.5. Source code in src\\metrics.py 58 59 60 61 62 63 64 65 def __init__ ( self , threshold = .5 ) -> None : \"\"\" Initializes the accuracy metric. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. \"\"\" self . threshold = threshold","title":"__init__"},{"location":"reference/#src.metrics.accuracy.compute","text":"Computes the accuracy of predictions. Parameters: y ( array ) \u2013 True labels (ground truth). p ( array ) \u2013 Predicted values. Returns: float ( float ) \u2013 accuracy value. Source code in src\\metrics.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def compute ( self , y : numpy . array , p : numpy . array ) -> float : \"\"\" Computes the accuracy of predictions. Args: y (numpy.array): True labels (ground truth). p (numpy.array): Predicted values. Returns: float: accuracy value. \"\"\" if y . shape [ 1 ] > 1 : p = p . argmax ( axis = 1 ) y = y . argmax ( axis = 1 ) else : p = ( p > self . threshold ) + 0 self . y , self . p = y , p return (( self . y == self . p ) . sum () / len ( self . y )) . round ( 4 )","title":"compute"},{"location":"reference/#section-7-database-management","text":"This modules provides sqlalchemy orm tables and utility objects DefaultTable - generic table template Architecture - Architecture table Layer - layer table Neurons - neurons table Cost - cost table Weight - weight table","title":"Section 7. database management"},{"location":"reference/#src.db.DBmanager","text":"Manages database connections and sessions using SQLAlchemy. Parameters: db ( str ) \u2013 Path to database server or SQLite database file. Defaults to None. Attributes: session ( Session ) \u2013 SQLAlchemy session for database operations. Methods: Name Description _DBmanager__start Starts a session add_table(table: DefaultTable) -> None: Adds a table instance to the current session. commit() -> None: Commits changes to the session. Source code in src\\db.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class DBmanager : \"\"\" Manages database connections and sessions using SQLAlchemy. Args: db (str, optional): Path to database server or SQLite database file. Defaults to None. Attributes: session (Session): SQLAlchemy session for database operations. Methods: _DBmanager__start()-> None: Starts a session add_table(table: DefaultTable) -> None: Adds a table instance to the current session. commit() -> None: Commits changes to the session. \"\"\" engines = {} status = False def __start ( db : str = None ) -> None : db_path = db or f 'sqlite:/// { get_module_path ([ \"run\" , f \"model { now () } .db\" ]) } ' DBmanager . path = db_path DBmanager . engines [ DBmanager . path ] = create_engine ( DBmanager . path ) Base . metadata . create_all ( DBmanager . engines [ DBmanager . path ]) Session = sessionmaker ( bind = DBmanager . engines [ DBmanager . path ]) DBmanager . session = Session () def add_table ( self , table : DefaultTable ) -> None : if not DBmanager . status : DBmanager . _DBmanager__start () DBmanager . status = True DBmanager . session . add ( table ) def commit ( self ) -> None : DBmanager . session . commit ()","title":"DBmanager"},{"location":"reference/#src.db.get_instance","text":"Returns an SQLAlchemy Table object corresponding to the given table name. Returns: Table ( DefaultTable ) \u2013 SQLAlchemy Table object corresponding to the specified table name. Source code in src\\db.py 70 71 72 73 74 75 76 77 78 79 def get_instance ( self ) -> DefaultTable : \"\"\" Returns an SQLAlchemy Table object corresponding to the given table name. Returns: Table: SQLAlchemy Table object corresponding to the specified table name. \"\"\" table , cols = tables [ str ( self )] values = { k : v for k , v in self . id . items () if k in cols } return table ( ** values )","title":"get_instance"},{"location":"reference/#src.db.update_instance","text":"Updates the given instance with the provided keyword arguments. Source code in src\\db.py 81 82 83 84 85 86 87 88 89 def update_instance ( self ) -> None : \"\"\" Updates the given instance with the provided keyword arguments. \"\"\" _ , cols = tables [ str ( self )] for k , v in self . id . items (): if k in cols : setattr ( self . table , k , v )","title":"update_instance"},{"location":"reference/#section-8-data-preparation-functions","text":"This modules provides functions for data preparation Batch - feed data in chunks shuffle - shuffles train sets onehot - onehot encodes target variables scaler - scales input features","title":"Section 8. data preparation functions"},{"location":"reference/#src.pipeline.Batch","text":"Source code in src\\pipeline.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class Batch : def __init__ ( self , size : int , obs : int , X : callable , y : callable ) -> None : \"\"\" Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.array): function providing access to Numpy array containing features. y (numpy.array): function providing access to Numpy array containing target variable. Returns: None Example: ```python >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] ``` \"\"\" self . size = size self . obs = obs self . X = X self . y = y self . getters = lambda ix : ( X ()[ ix ,:], y ()[ ix ,:]) self . i = self . getters ( slice ( 0 , 10 )) self . ix = get_ix ( size , obs ) self . c = 0 def __iter__ ( self ): return self def __next__ ( self ): if self . c < len ( self . ix ): self . c += 1 return self . getters ( self . ix [ self . c - 1 ]) self . c = 0 raise StopIteration","title":"Batch"},{"location":"reference/#src.pipeline.Batch.__init__","text":"Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.array): function providing access to Numpy array containing features. y (numpy.array): function providing access to Numpy array containing target variable. Returns: None Example: >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] Source code in src\\pipeline.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , size : int , obs : int , X : callable , y : callable ) -> None : \"\"\" Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.array): function providing access to Numpy array containing features. y (numpy.array): function providing access to Numpy array containing target variable. Returns: None Example: ```python >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] ``` \"\"\" self . size = size self . obs = obs self . X = X self . y = y self . getters = lambda ix : ( X ()[ ix ,:], y ()[ ix ,:]) self . i = self . getters ( slice ( 0 , 10 )) self . ix = get_ix ( size , obs ) self . c = 0","title":"__init__"},{"location":"reference/#src.pipeline.get_ix","text":"Create batch slices for a given sample size and batch size. Parameters: obs ( int ) \u2013 Total number of samples in the dataset. size ( int ) \u2013 Size of each batch. Returns: list [ slice ] \u2013 list[slice]: A list of slice objects representing batch indices. Example: >>> obs = 70 # Total samples >>> batch_size = 20 >>> batch_slices = get_ix(obs, batch_size) >>> batch_slices [slice(0, 20, None), slice(20, 40, None), slice(40, 60, None), slice(60, 70, None)] Source code in src\\pipeline.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def get_ix ( size : int , obs : int ) -> list [ slice ]: \"\"\" Create batch slices for a given sample size and batch size. Args: obs (int): Total number of samples in the dataset. size (int): Size of each batch. Returns: list[slice]: A list of slice objects representing batch indices. Example: ```python >>> obs = 70 # Total samples >>> batch_size = 20 >>> batch_slices = get_ix(obs, batch_size) >>> batch_slices [slice(0, 20, None), slice(20, 40, None), slice(40, 60, None), slice(60, 70, None)] ``` \"\"\" batchix = list ( range ( 0 , obs , size )) if batchix [ - 1 ] < obs : batchix . append ( obs ) batchix = [ slice ( low , high ) for low , high in zip ( batchix , batchix [ 1 :])] return batchix","title":"get_ix"},{"location":"reference/#src.pipeline.onehot","text":"One-hot encodes a categorical target variable. Parameters: y ( array ) \u2013 Numpy array containing the categorical target variable. Returns: array \u2013 numpy.array: One-hot encoded representation of the target variable. Example: >>> y = numpy.array([[0],[ 1], [2], [1], [0]]) >>> onehot_encoded = onehot(y) >>> print(onehot_encoded) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] Source code in src\\pipeline.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def onehot ( y : numpy . array ) -> numpy . array : \"\"\" One-hot encodes a categorical target variable. Args: y (numpy.array): Numpy array containing the categorical target variable. Returns: numpy.array: One-hot encoded representation of the target variable. Example: ```python >>> y = numpy.array([[0],[ 1], [2], [1], [0]]) >>> onehot_encoded = onehot(y) >>> print(onehot_encoded) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] ``` \"\"\" return ( y == numpy . unique ( y )) + 0","title":"onehot"},{"location":"reference/#src.pipeline.scaler","text":"Custom scaler function for centering and standardizing features. Parameters: X ( array ) \u2013 Input numpy array containing features. Returns: array \u2013 numpy.array: Scaled version of the input array. Example: >>> X = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> scaled_X = scaler(X) >>> print(scaled_X) [[-1.22474487 -1.22474487] [ 0. 0. ] [ 1.22474487 1.22474487]] Source code in src\\pipeline.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def scaler ( X : numpy . array ) -> numpy . array : \"\"\" Custom scaler function for centering and standardizing features. Args: X (numpy.array): Input numpy array containing features. Returns: numpy.array: Scaled version of the input array. Example: ```python >>> X = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> scaled_X = scaler(X) >>> print(scaled_X) [[-1.22474487 -1.22474487] [ 0. 0. ] [ 1.22474487 1.22474487]] ``` \"\"\" return ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 )","title":"scaler"},{"location":"reference/#src.pipeline.shuffle","text":"shuffle features and tagert variable numpy arrays X and y using pandas.sample method. Parameters: X ( array ) \u2013 Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y ( array ) \u2013 Target variable with shape (n, 1). Returns: tuple [ array , array ] \u2013 Tuple[numpy.array, numpy.array]: Shuffled X and y arrays. Example: >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> shuffled_X, shuffled_y = shuffle(X_train, y_train) # Now shuffled_X and shuffled_y contain randomly shuffled samples. Source code in src\\pipeline.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def shuffle ( X : numpy . array , y : numpy . array ) -> tuple [ numpy . array , numpy . array ]: \"\"\" shuffle features and tagert variable numpy arrays X and y using pandas.sample method. Args: X (numpy.array): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.array): Target variable with shape (n, 1). Returns: Tuple[numpy.array, numpy.array]: Shuffled X and y arrays. Example: ```python >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> shuffled_X, shuffled_y = shuffle(X_train, y_train) # Now shuffled_X and shuffled_y contain randomly shuffled samples. ``` \"\"\" X = pandas . DataFrame ( X ) . sample ( frac = 1 ) y = pandas . DataFrame ( y ) . loc [ X . index ] return X . values , y . values","title":"shuffle"},{"location":"reference/#9-class-models-used-to-build-other-classes","text":"This modules provides genric templates for other modules Define - generic object Architecture - Architecture super object Layer - layer super object Neurons - neurons super object Cost - cost super object Metrics - weight super object","title":"9. Class Models used to build other classes"},{"location":"reference/#src.model.Architecture","text":"Bases: Define Model for Architecture functions see :func: ~architecture.Sequential Source code in src\\model.py 316 317 318 319 320 321 322 323 class Architecture ( Define ): \"\"\" Model for Architecture functions see :func:`~architecture.Sequential` \"\"\" def __str__ ( self ) -> str : return 'Architecture'","title":"Architecture"},{"location":"reference/#src.model.Cost","text":"Bases: Define Model for Cost functions see :func: ~cost.binaryCrossEntropy Source code in src\\model.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 class Cost ( Define ): \"\"\" Model for Cost functions see :func:`~cost.binaryCrossEntropy` \"\"\" def clip ( self ): \"\"\" Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of ``[0, 1]`` is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``. No check is performed to ensure ``a_min < a_max``. Parameters ---------- a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If ``None``, clipping is not performed on the corresponding edge. Only one of `a_min` and `a_max` may be ``None``. Both are broadcast against `a`. out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. `out` must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`. .. versionadded:: 1.17.0 Returns ------- clipped_array : ndarray An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`. See Also -------- :ref:`ufuncs-output-type` Notes ----- When `a_min` is greater than `a_max`, `clip` returns an array in which all values are equal to `a_max`, as shown in the second example. Examples -------- ```python >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) ``` \"\"\" \u03b5 = 1e-7 self . p = self . p . clip ( \u03b5 , 1 - \u03b5 ) def __str__ ( self ) -> str : return 'Cost'","title":"Cost"},{"location":"reference/#src.model.Cost.clip","text":"Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than np.minimum(a_max, np.maximum(a, a_min)) . No check is performed to ensure a_min < a_max .","title":"clip"},{"location":"reference/#src.model.Cost.clip--parameters","text":"a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If None , clipping is not performed on the corresponding edge. Only one of a_min and a_max may be None . Both are broadcast against a . out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. out must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref: ufunc docs <ufuncs.kwargs> . .. versionadded:: 1.17.0","title":"Parameters"},{"location":"reference/#src.model.Cost.clip--returns","text":"clipped_array : ndarray An array with the elements of a , but where values < a_min are replaced with a_min , and those > a_max with a_max .","title":"Returns"},{"location":"reference/#src.model.Cost.clip--see-also","text":":ref: ufuncs-output-type","title":"See Also"},{"location":"reference/#src.model.Cost.clip--notes","text":"When a_min is greater than a_max , clip returns an array in which all values are equal to a_max , as shown in the second example.","title":"Notes"},{"location":"reference/#src.model.Cost.clip--examples","text":">>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) Source code in src\\model.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def clip ( self ): \"\"\" Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of ``[0, 1]`` is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``. No check is performed to ensure ``a_min < a_max``. Parameters ---------- a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If ``None``, clipping is not performed on the corresponding edge. Only one of `a_min` and `a_max` may be ``None``. Both are broadcast against `a`. out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. `out` must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`. .. versionadded:: 1.17.0 Returns ------- clipped_array : ndarray An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`. See Also -------- :ref:`ufuncs-output-type` Notes ----- When `a_min` is greater than `a_max`, `clip` returns an array in which all values are equal to `a_max`, as shown in the second example. Examples -------- ```python >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) ``` \"\"\" \u03b5 = 1e-7 self . p = self . p . clip ( \u03b5 , 1 - \u03b5 )","title":"Examples"},{"location":"reference/#src.model.Define","text":"Bases: DBmanager Source code in src\\model.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class Define ( DBmanager ): def __repr__ ( self ) -> str : \"\"\" Returns the name of the class. Returns: str: The name of the class. \"\"\" return self . __class__ . __name__ @property def id ( self ) -> dict : return self . _id @id . setter def id ( self , loc ) -> None : \"\"\" Sets id property for the class Instantiate and stores sqlalchemy table of self \"\"\" loc = unfold ( loc ) self . _id = { 'id' : id ( self ), f ' { str ( self ) } _id' : id ( self ), 'name' : repr ( self ), ** loc } if not hasattr ( self , 'table' ): self . table = get_instance ( self ) self . add_table ( self . table ) else : update_instance ( self ) self . commit () def __getitem__ ( self , ix ) -> any : return self . id [ ix ] def __setitem__ ( self , ix , val ) -> None : self . id [ ix ] = val @property def get ( self ) -> dict . get : return self . id . get def __add__ ( self , loc : dict ) -> None : \"\"\" Triggers the id property Args: loc (dict) : dictionary of properties \"\"\" self . id = loc self . c = 0 class func : def __init__ ( self , _ ): ... self . init_method = self . get ( 'Layer_init_method' , func ) self . func = self . get ( 'func' , func ) self . func = self . func ( self . id ) self [ 'steps' ] = self . get ( 'steps' ,[]) parent = { f ' { str ( self ) } _id' : self [ 'id' ]} for step in self : step . id = { ** step . id , ** parent } def __iter__ ( self ) -> object : return self def __len__ ( self ) -> int : return len ( self [ 'steps' ]) def __next__ ( self ) -> any : if self . c < len ( self ): self . c += 1 return self [ 'steps' ][ self . c - 1 ] self . c = 0 raise StopIteration def predict ( self , X : numpy . array ) -> numpy . array : \"\"\" Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Args: X (numpy.array) : input features matrix Returns: numpy.array of output layer predictions \"\"\" self . out = X for step in self : self . out = step . func . compute ( self . out ) return self . out def update ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Implement backpropagation of cost gradient to all layers Passes gradients backward Args: \u0394 (numpy.array) : array of gradient from next step Returns: numpy array of input layer gradient \"\"\" for step in self [ 'steps' ][:: - 1 ]: \u0394 = step . func . grad ( \u0394 ) return \u0394 def compute_store ( self ) -> None : \"\"\" Generic method that computes item and stores it to sqlalchemy session \"\"\" value = self . compute ( self . y , self . p ) self . commit () del ( self . table ) self + { ** self . id , ** locals ()} return value def updateW ( self ) -> None : \"\"\" Updates sqlalchemy tables containing weights \"\"\" for obj in Neurons . with_weights : for i , r in enumerate ( obj . Wtables ): for j , table in enumerate ( r ): setattr ( table , 'value' , obj . W [ i , j ])","title":"Define"},{"location":"reference/#src.model.Define.__add__","text":"Triggers the id property Parameters: loc ( dict) ) \u2013 dictionary of properties Source code in src\\model.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __add__ ( self , loc : dict ) -> None : \"\"\" Triggers the id property Args: loc (dict) : dictionary of properties \"\"\" self . id = loc self . c = 0 class func : def __init__ ( self , _ ): ... self . init_method = self . get ( 'Layer_init_method' , func ) self . func = self . get ( 'func' , func ) self . func = self . func ( self . id ) self [ 'steps' ] = self . get ( 'steps' ,[]) parent = { f ' { str ( self ) } _id' : self [ 'id' ]} for step in self : step . id = { ** step . id , ** parent }","title":"__add__"},{"location":"reference/#src.model.Define.__repr__","text":"Returns the name of the class. Returns: str ( str ) \u2013 The name of the class. Source code in src\\model.py 17 18 19 20 21 22 23 24 def __repr__ ( self ) -> str : \"\"\" Returns the name of the class. Returns: str: The name of the class. \"\"\" return self . __class__ . __name__","title":"__repr__"},{"location":"reference/#src.model.Define.compute_store","text":"Generic method that computes item and stores it to sqlalchemy session Source code in src\\model.py 124 125 126 127 128 129 130 131 132 133 def compute_store ( self ) -> None : \"\"\" Generic method that computes item and stores it to sqlalchemy session \"\"\" value = self . compute ( self . y , self . p ) self . commit () del ( self . table ) self + { ** self . id , ** locals ()} return value","title":"compute_store"},{"location":"reference/#src.model.Define.predict","text":"Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Parameters: X ( numpy.array) ) \u2013 input features matrix Returns: array \u2013 numpy.array of output layer predictions Source code in src\\model.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def predict ( self , X : numpy . array ) -> numpy . array : \"\"\" Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Args: X (numpy.array) : input features matrix Returns: numpy.array of output layer predictions \"\"\" self . out = X for step in self : self . out = step . func . compute ( self . out ) return self . out","title":"predict"},{"location":"reference/#src.model.Define.update","text":"Implement backpropagation of cost gradient to all layers Passes gradients backward Parameters: \u0394 ( numpy.array) ) \u2013 array of gradient from next step Returns: array \u2013 numpy array of input layer gradient Source code in src\\model.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def update ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Implement backpropagation of cost gradient to all layers Passes gradients backward Args: \u0394 (numpy.array) : array of gradient from next step Returns: numpy array of input layer gradient \"\"\" for step in self [ 'steps' ][:: - 1 ]: \u0394 = step . func . grad ( \u0394 ) return \u0394","title":"update"},{"location":"reference/#src.model.Define.updateW","text":"Updates sqlalchemy tables containing weights Source code in src\\model.py 135 136 137 138 139 140 141 142 143 def updateW ( self ) -> None : \"\"\" Updates sqlalchemy tables containing weights \"\"\" for obj in Neurons . with_weights : for i , r in enumerate ( obj . Wtables ): for j , table in enumerate ( r ): setattr ( table , 'value' , obj . W [ i , j ])","title":"updateW"},{"location":"reference/#src.model.Layer","text":"Bases: Define Model for layer functions see :func: ~layer.fullyconnected Source code in src\\model.py 145 146 147 148 149 150 151 152 class Layer ( Define ): \"\"\" Model for layer functions see :func:`~layer.fullyconnected` \"\"\" def __str__ ( self ) -> str : return 'Layer'","title":"Layer"},{"location":"reference/#src.model.Metrics","text":"Bases: Define Model for Metrics functions see :func: ~metrics.accuracy Source code in src\\model.py 307 308 309 310 311 312 313 314 class Metrics ( Define ): \"\"\" Model for Metrics functions see :func:`~metrics.accuracy` \"\"\" def __str__ ( self ) -> str : return 'Metrics'","title":"Metrics"},{"location":"reference/#src.model.Neurons","text":"Bases: Define Model for activation functions see :func: ~activation.Softmax Source code in src\\model.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 class Neurons ( Define ): \"\"\" Model for activation functions see :func:`~activation.Softmax` \"\"\" with_weights = [] def instantiateW ( self ) -> None : \"\"\" Instantiate weight tables \"\"\" table , cols = tables [ 'Weight' ] self . Wtables = [] for i , r in enumerate ( self . W ): instances = [] for j , e in enumerate ( r ): instances += [ table ( Weight_id = f ' { i } _ { j } ' , value = e , Neurons_id = self [ 'id' ] ) ] self . Wtables += [ instances ] instances = [] Neurons . with_weights += [ self ] def storeW ( self ): \"\"\" Stores weights tables \"\"\" for row in self . Wtables : for table in row : self . add_table ( table ) def __str__ ( self ) -> str : return 'Neurons' def __sub__ ( self , \u0394 : numpy . array ) -> None : \"\"\" Substracts Gradient to Weights \"\"\" self . W -= \u0394 def n ( self ) -> int : \"\"\" Returns sample size for current features matrix \"\"\" return self . X . shape [ 0 ] def grad ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Computes gradient for previous step Args: \u0394 (numpy.array) : gradient from next step Returns: numpy.array of gradient for previous step \"\"\" self . \u0394 = self . pr () * \u0394 return self . \u0394","title":"Neurons"},{"location":"reference/#src.model.Neurons.__sub__","text":"Substracts Gradient to Weights Source code in src\\model.py 194 195 196 197 198 199 def __sub__ ( self , \u0394 : numpy . array ) -> None : \"\"\" Substracts Gradient to Weights \"\"\" self . W -= \u0394","title":"__sub__"},{"location":"reference/#src.model.Neurons.grad","text":"Computes gradient for previous step Parameters: \u0394 ( numpy.array) ) \u2013 gradient from next step Returns: array \u2013 numpy.array of gradient for previous step Source code in src\\model.py 208 209 210 211 212 213 214 215 216 217 218 219 220 def grad ( self , \u0394 : numpy . array ) -> numpy . array : \"\"\" Computes gradient for previous step Args: \u0394 (numpy.array) : gradient from next step Returns: numpy.array of gradient for previous step \"\"\" self . \u0394 = self . pr () * \u0394 return self . \u0394","title":"grad"},{"location":"reference/#src.model.Neurons.instantiateW","text":"Instantiate weight tables Source code in src\\model.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def instantiateW ( self ) -> None : \"\"\" Instantiate weight tables \"\"\" table , cols = tables [ 'Weight' ] self . Wtables = [] for i , r in enumerate ( self . W ): instances = [] for j , e in enumerate ( r ): instances += [ table ( Weight_id = f ' { i } _ { j } ' , value = e , Neurons_id = self [ 'id' ] ) ] self . Wtables += [ instances ] instances = [] Neurons . with_weights += [ self ]","title":"instantiateW"},{"location":"reference/#src.model.Neurons.n","text":"Returns sample size for current features matrix Source code in src\\model.py 201 202 203 204 205 def n ( self ) -> int : \"\"\" Returns sample size for current features matrix \"\"\" return self . X . shape [ 0 ]","title":"n"},{"location":"reference/#src.model.Neurons.storeW","text":"Stores weights tables Source code in src\\model.py 183 184 185 186 187 188 189 def storeW ( self ): \"\"\" Stores weights tables \"\"\" for row in self . Wtables : for table in row : self . add_table ( table )","title":"storeW"},{"location":"reference/#10-utility-functions","text":"Provides utility functions get_module_path - Returns the path to a subdirectory named 'dir' relative to the currently executed script. now - current timestamp unfold - Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary.","title":"10. Utility functions"},{"location":"reference/#src.utils.get_module_path","text":"Returns the path to a subdirectory named 'dir' relative to the currently executed script. Parameters: dir ( str ) \u2013 path to the subdirectory. Returns: str ( str ) \u2013 Absolute path to the specified subdirectory. Source code in src\\utils.py 12 13 14 15 16 17 18 19 20 21 22 def get_module_path ( dir : list [ str ]) -> str : \"\"\" Returns the path to a subdirectory named 'dir' relative to the currently executed script. Args: dir (str): path to the subdirectory. Returns: str: Absolute path to the specified subdirectory. \"\"\" return os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), * dir )","title":"get_module_path"},{"location":"reference/#src.utils.now","text":"Returns the current timestamp as an integer. Returns: int ( int ) \u2013 Current timestamp (number of seconds since the epoch). Source code in src\\utils.py 24 25 26 27 28 29 30 31 def now () -> int : \"\"\" Returns the current timestamp as an integer. Returns: int: Current timestamp (number of seconds since the epoch). \"\"\" return int ( datetime . datetime . now () . timestamp ())","title":"now"},{"location":"reference/#src.utils.unfold","text":"Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. Parameters: d ( dict ) \u2013 Input dictionary with nested dictionaries. Returns: dict ( dict ) \u2013 Unfolded dictionary with concatenated keys. Example: >>> d = {'a':1,'b':{'c':2,'d':4}} >>> unfold(d) {'a': 1, 'b_c': 2, 'b_d': 4} Source code in src\\utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def unfold ( d : dict ) -> dict : \"\"\" Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. Args: d (dict): Input dictionary with nested dictionaries. Returns: dict: Unfolded dictionary with concatenated keys. Example: ```python >>> d = {'a':1,'b':{'c':2,'d':4}} >>> unfold(d) {'a': 1, 'b_c': 2, 'b_d': 4} ``` \"\"\" new_d = {} for k in d : if hasattr ( d [ k ], 'keys' ): for j in d [ k ]: new_d [ f ' { k } _ { j } ' ] = d [ k ][ j ] else : new_d [ k ] = d [ k ] return new_d","title":"unfold"}]}